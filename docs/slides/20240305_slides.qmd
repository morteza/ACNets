---
title: Improving AVGP Classification
author: "Morteza, Pedro"
format: revealjs
---

## Introduction


::: {.notes}
Hello we uh. want to understand the relationship between the brain function and behavior And mainly we are interested in action video gamers versus non gamers because it's a trait that is different in behavior and cognition and that is probably depend on the because of the brain something different changes in the brain. How can we determine that the difference in the brain causes the differences in the action video gamers versus non gamers in behavior and cognition Because of the "HOW", this is the method question because of the how.

- Briefly introduce the goal: understanding the relationship between brain function and behavior.
- Highlight the case study: action video gamers vs. non-gamers.
:::

## Recipe 1: standard approach

::: {.notes}
So the standard method is using something like glm, linear models, T-tests and all these  methods from statistics. What we do in this category of methods is that we rely on previous literature to identify cognitive constructs that action video and non gamers differ and then test for relevant regions and networks.The way we do it is that we figure out some regions or networks And we then do some calculation and if the difference between groups are significantly higher from chance than the feature/region/network is considered important for the main difference between groups.

These methods are pretty informative if considered as providing a starting point for further investigation. However, they may also suffer one important thing:
- feature importance is taken for granted as it comes from previous literature or it's combined with significance testing. So anything that is better than chance is considered important, an we might not be able to generalize to unseen data. This makes it pruned to  subjectivity and spurious findings. The method is also limited in terms of generalization to unseen data.

:::

## Hypothesis 2: ML (Accuracy vs. Interpretability)

::: {.notes}

How can we improve that?

- Machine learning:
- Difficulty in interpreting results due to lack of semantic and context of the brain regions.
- Inconsistency in achieving high classification accuracy.


The second method is using like machine learning, like cross validation, those kind of things. And we want here to find a robust method that allows us to predict out of sample Um subjects like for example, can be generalized the results to the subjects that we don't see. Like here, we can try different methods like different. preprocessing different connectivity measures different machine learning. The problem Uh that here I wanted to mail the address is that like we want to We want semantic and context And because of that, maybe it's better to study networks, because that's the part of the brain. That's the function of the brain that We can usually understand like I will like. I will cite here the paper by pull track that. the reverse imprints that What we did in the previous like in this study was that I ran a classification accuracy classification task on the data set that we had, and then performed feature importance So like the results are a bit inconsistent. Like, for example, like we can see that the model that produces high accuracy here is the combining connectivity metrics based on networks. It produces 75% of accuracy And this is the only result I've seen like I'm quoting Tolstoy here. So the first hypothesis is that, like for interpretation of the results First, the strategy to achieve the high classification accuracy because the model here the inconsistency could be because The inconsistency here could be because All the models that we have here The results show the inconsistency and this could be the first hypothesis is that it could be because the accuracy is done that high So can we reach higher accuracy? This mainly involve feature engineering at the as the like the SVM changing the classification, changing the features that raw features we are using. And all these inconsistency could be because we are not reaching like a lower band of accuracy for reliable results. This is the first hypothesis. What happened is that like when I try to reach accuracy, this is 75% Uh, I couldn't see any reliable results because if we change the. Classic. If we feed all the features, if we like do different things, there's also. not reliable anyway. Depends on the different metallogic choices that we have.
:::

- Inconsistencies in results might be due to focusing solely on high accuracy.
- Explore alternative approaches to achieve reliable interpretation.


## Hypothesis 3: Data Limitations

::: {.notes}

Maybe we don't have proper data like enough data. The one is that number of participants, number of samples that we have, the duration of. A resting state that we have. And we might have noise, we might have noise in the labeling. So for example, if someone is like expert, but we don't consider him or her as some people might be, experts in some Similar domain, but we don't consider them as actually the gamer. The problem here would be, even if you have enough data, if you have large data set, it doesn't really solve the other fields like it could solve action video gamers. But then the question would be, how about I don't know, other diseases, other traits So the question remains for other fields and we cannot really collect that much of data. The other problem with this like higher data, the challenge is that like we don't really rely on the past. Information like, for example, like in the modeling in the method that we are using, we start from blank slate. 

:::

- Potential limitations: insufficient data, noise, labeling errors.
- Increasing data size might not always solve problems in other fields.


## Hypothesis 4: Pre-training and Transfer Learning

::: {.notes}
A second hypothesis or a second method is like pre-training. So we have these past data and maybe. we can use methods to incorporate past data and learn more efficient Efficiently on the smaller data sets. So for example here is the birds and we cannot really learn birds based on 2 3 data set, but it's been shown that if we train and it train the data set unlike. Father, huge data set, unsupervised way, then. we can make supervised learning a bit better. High performance. In fact, without this pre training, it's an impossible task to learn in a supervised way, like with labels So what I've done is like I've tried different architectures. The main idea was like, we can combine like eggs and Fmri's in a single. Cohesive architecture With different inputs and outputs we can like use all these data from open narrow. These are all resting states. We have a few data very much so many data sets that use resting state and then we can fine unit. And the smaller data sets. So this is good because of the multiple reasons why if we can like use instead of like using Here, for example, 24 subjects for training and eight subjects for testing. We can use half, half or a smaller training. It's also much faster than we do. not need to run it like for a long, very long time. So we can use like for me my personal computer to run this fine tuning. So here's some of the architectures. 
:::

- Leverage existing large datasets to improve performance on smaller datasets.
- Combine fMRI data with other relevant data sources (e.g., behavioral data).
- Utilize transfer learning to fine-tune models on smaller datasets.

## Benefits of the transfer learning
- Improved efficiency and interpretability.
- Reduced data collection requirements, and smaller test.
- Faster model fitting.
- Multimodality and generalization to other questions.

## Conclusion
- Summarize key points and potential impact of the proposed solution.
