{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connectivity Classifier\n",
    "\n",
    "This notebook trains a binary classifier to predict the group of a participant (AVGP or NVGP) based on their connectivity matrices.\n",
    "\n",
    "It implements the following steps:\n",
    "\n",
    "1. Load the data\n",
    "2. Cross-validated classification pipeline\n",
    "3. Permutation testing\n",
    "4. Permutation importance\n",
    "5. SHAP\n",
    "6. Learning curve analysis\n",
    "\n",
    "\n",
    "## Inputs\n",
    "\n",
    "Region-level time-series are extracted using several possible parcellation atlases (i.e., DiFuMo64, Dosenbach2010, Gordon2014, Friedman2020, and Seitzman2018), and several aggregation (regions, networks, and randomized network assignment). The timeseries are then used to calculate connectivity matrices including correlation, partial correlation, tangent, precision, and covariance.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "Prediction accuracies on the test set for each combination of connectivity metric, parcellation, and aggregation mode. The results are stored in the following file:\n",
    "  - `models/connectivity_classifier_*.nc5`\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "To run this notebook, you need to have a few packages installed. The easiest way to do this is to use mamba to create a new environment from the `environment.yml` file in the root of this repository:\n",
    "\n",
    "```bash\n",
    "mamba env create -f environment.yml\n",
    "mamba activate acnets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# 0. SETUP\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from src.acnets.pipeline import ConnectivityPipeline, ConnectivityVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import (GridSearchCV, StratifiedShuffleSplit,\n",
    "                                     cross_val_score, learning_curve,\n",
    "                                     permutation_test_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from src.acnets.pipeline import Parcellation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "These parameters can be set in the command line when running the notebook, or in the notebook itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "N_CV_SPLITS = 10                    # number of cross-validation splits\n",
    "N_TEST_SUBJECTS = 8                 # test size for cross-validation (number of subjects)\n",
    "CV = StratifiedShuffleSplit(n_splits=N_CV_SPLITS, test_size=N_TEST_SUBJECTS)\n",
    "\n",
    "N_PERMUTATIONS = 10                 # for permutation test\n",
    "\n",
    "# Analysis flags\n",
    "ENABLE_SHAP = False                     # run SHAP analysis or not as it takes a long time\n",
    "ENABLE_PERMUTATION_TEST = False         # run permutation test or not as it takes a long time\n",
    "ENABLE_LEARNING_CURVE = False           # run learning curve or not as it takes a long time\n",
    "ENABLE_PERMUTATION_IMPORTANCE = False   # run permutation importance or not as it takes a long time\n",
    "\n",
    "MODELS_DIR= Path('models/')              # Directory to save models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Here we load the data from the `data/julia2018/` dataset. These files contain the connectivity matrices for each participant, for each combination of parcellation and connectivity metric. For the reminder of this notebook, we only focus on `dosenbach2010` parcellation atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# DATA PREPARATION\n",
    "parcellation = Parcellation(atlas_name='dosenbach2010').fit()\n",
    "\n",
    "subjects = parcellation.dataset_.coords['subject'].values\n",
    "\n",
    "# extract group labels (AVGP or NVGP) from subject ids (e.g. AVGP-01)\n",
    "subject_labels = [s[:4] for s in subjects]  \n",
    "\n",
    "X = subjects.reshape(-1, 1)  # subject ids, shape: (n_subjects, 1)\n",
    "\n",
    "y_encoder = LabelEncoder()\n",
    "y = y_encoder.fit_transform(subject_labels)     # labels, shape: (n_subjects,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# PREPARE OUTPUT DATASET FILE\n",
    "\n",
    "n_splits = CV.get_n_splits()\n",
    "n_folds = int(X.shape[0] / CV.test_size)\n",
    "\n",
    "model_output_name = ('connectivities'\n",
    "                     '_classifier-SVM'\n",
    "                     '_measure-accuracy'\n",
    "                     f'_cv-{n_splits}x{n_folds}fold.nc5'\n",
    "                     )\n",
    "\n",
    "OUTPUT_PATH = MODELS_DIR / model_output_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Maps\n",
    "\n",
    "We aggregate the region-level time-series using different strategies. The first strategy is use region time-series (no chance). The second strategy is to average the all region-level time-series across each network. The third strategy is to randomly assign each region to a network, and then average the time-series matrices across each network (random network assignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy 1: region to network\n",
    "region_to_network = parcellation.labels_['network'].to_dict()\n",
    "\n",
    "# strategy 2: region to region\n",
    "region_to_region = {k: k for k in region_to_network.keys()}  \n",
    "\n",
    "# convert them to dataframes\n",
    "region_to_network = pd.DataFrame.from_dict(region_to_network, orient='index',columns=['group'])\n",
    "region_to_region = pd.DataFrame.from_dict(region_to_region, orient='index', columns=['group'])\n",
    "\n",
    "# strategy 3: random network assignment\n",
    "region_to_random_network = region_to_network.copy().rename(columns={'group': 'original_group'})\n",
    "region_to_random_network['group'] = region_to_random_network['original_group'].sample(frac=1).values\n",
    "\n",
    "aggregation_strategies = {\n",
    "    'region': region_to_region,\n",
    "    'network': region_to_network,\n",
    "    'random-network': region_to_random_network\n",
    "}\n",
    "\n",
    "# set names for aggregation strategies; this will be handy later\n",
    "for k, v in aggregation_strategies.items():\n",
    "    v.name = k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "The pipeline is composed of the following steps:\n",
    "\n",
    "1. Extract connectivity matrices from the data\n",
    "2. Vectorize the connectivity matrices\n",
    "3. Scale the connectivity matrices\n",
    "4. Remove zero-variance features\n",
    "5. Select the top 32 features based on the coefficient of a SVM classifier\n",
    "6. SVM binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] overfit accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "# DEFINE PIPELINE\n",
    "\n",
    "pipe  = Pipeline([\n",
    "    ('connectivity', ConnectivityPipeline(region_to_network=region_to_network, kind='partial correlation')),\n",
    "    ('vectorize', ConnectivityVectorizer()),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('zerovar', VarianceThreshold()),\n",
    "    ('select', SelectFromModel(LinearSVC(penalty='l1', dual=False, max_iter=10000),\n",
    "                               max_features=lambda x: min(10, x.shape[1]))),\n",
    "    ('clf', LinearSVC(penalty='l1', dual=False, max_iter=10000))\n",
    "    # ('clf', SVC(kernel='linear', C=1))\n",
    "])\n",
    "\n",
    "# DEBUG (expected to overfit, i.e., score=1)\n",
    "overfit_score = pipe.fit(X, y).score(X, y)\n",
    "print(f'[DEBUG] overfit accuracy: {overfit_score:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the pipeline\n",
    "\n",
    "Here we verify that the pipeline works by running it on all aggregation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06f05e0007d433899db70ef7738d31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[region]\n",
      "Test accuracy (mean ± std): 0.50 ± 0.23\n",
      "ConfidenceInterval(low=0.3375, high=0.625) \n",
      "\n",
      "[network]\n",
      "Test accuracy (mean ± std): 0.69 ± 0.13\n",
      "ConfidenceInterval(low=0.6125, high=0.7625) \n",
      "\n",
      "[random-network]\n",
      "Test accuracy (mean ± std): 0.59 ± 0.15\n",
      "ConfidenceInterval(low=0.5125, high=0.7125) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VERIFY THE MODEL (calculate cross-validated accuracy and bootstrap CI)\n",
    "\n",
    "for agg_strategy, agg_mapping in tqdm(aggregation_strategies.items()):\n",
    "\n",
    "    pipe.set_params(connectivity__atlas='dosenbach2010',\n",
    "                    connectivity__kind='partial correlation',\n",
    "                    connectivity__region_to_network=agg_mapping)\n",
    "\n",
    "    scores = cross_val_score(pipe, X, y,\n",
    "                            cv=CV,\n",
    "                            scoring='accuracy',\n",
    "                            n_jobs=-1)\n",
    "    bootstrap_ci = stats.bootstrap(scores.reshape(1,-1), np.mean)\n",
    "\n",
    "    print(f'[{agg_strategy}]')\n",
    "    print('Test accuracy (mean ± std): {:.2f} ± {:.2f}'.format(scores.mean(), scores.std()))\n",
    "    print(bootstrap_ci.confidence_interval, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best estimator: Pipeline(steps=[('connectivity',\n",
      "                 ConnectivityPipeline(atlas='dosenbach2010',\n",
      "                                      kind='partial correlation',\n",
      "                                      bids_dir='data/julia2018',\n",
      "                                      parcellation_cache_dir='data/julia2018/derivatives/resting_timeseries/',\n",
      "                                      region_to_network=                     original_group              group\n",
      "vmPFC 1                     default            default\n",
      "aPFC 2              fronto-parietal          occipital\n",
      "aPFC 3              fronto-parietal  cingulo-opercular\n",
      "mPFC 4                      default         cerebellum\n",
      "aPFC 5                      d...\n",
      "post occipital 159        occipital  cingulo-opercular\n",
      "post occipital 160        occipital            default\n",
      "\n",
      "[160 rows x 2 columns])),\n",
      "                ('vectorize', ConnectivityVectorizer()),\n",
      "                ('scale', StandardScaler()), ('zerovar', VarianceThreshold()),\n",
      "                ('select',\n",
      "                 SelectFromModel(estimator=LinearSVC(dual=False, max_iter=10000,\n",
      "                                                     penalty='l1'),\n",
      "                                 max_features=<function <lambda> at 0x7f12632148b0>)),\n",
      "                ('clf', LinearSVC(dual=False, max_iter=10000, penalty='l1'))]) \n",
      " best score: 0.725\n"
     ]
    }
   ],
   "source": [
    "# RUN PIPELINE ON ALL METRICS\n",
    "\n",
    "param_grid = {\n",
    "    'connectivity__region_to_network': list(aggregation_strategies.values()),\n",
    "    'connectivity__atlas': ['dosenbach2010'],\n",
    "    # 'connectivity__atlas': ['dosenbach2010', 'gordon2014_2mm', 'difumo_64_2mm'],\n",
    "    'connectivity__kind': ['partial correlation', 'correlation', 'covariance', 'precision', 'tangent'],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=CV,\n",
    "    verbose=1,\n",
    "    n_jobs=-2,\n",
    "    scoring='accuracy')\n",
    "\n",
    "grid.fit(X, y)\n",
    "clear_output(wait=True)\n",
    "\n",
    "\n",
    "print('best estimator:', grid.best_estimator_, '\\n', 'best score:', grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO rewrite the rest of the code to support the new aggregation strategy\n",
    "\n",
    "def get_model_name(params):\n",
    "    \"\"\"Helper function to generate a model name from the parameters.\"\"\"\n",
    "\n",
    "    atlas_name = params['connectivity__atlas']\n",
    "    agg_name = params['connectivity__region_to_network']\n",
    "    kind_name = params['connectivity__kind']\n",
    "    name = f'{atlas_name}_{agg_name}_{kind_name}'\n",
    "\n",
    "    return name\n",
    "\n",
    "mappings = []\n",
    "for c in grid.cv_results_['params']:\n",
    "    if type(c['connectivity__region_to_network']) == pd.DataFrame:\n",
    "        c['connectivity__region_to_network'] = c['connectivity__region_to_network'].name\n",
    "    mappings.append(c['connectivity__region_to_network'])\n",
    "grid.cv_results_['param_connectivity__region_to_network'] = mappings\n",
    "\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results['model_name'] = results['params'].apply(get_model_name)\n",
    "\n",
    "results = results.sort_values(by='mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERMUTATION TEST (SHUFFLE Y)\n",
    "\n",
    "# if not ENABLE_PERMUTATION_TEST:\n",
    "#     raise ValueError('ENABLE_PERMUTATION_TEST must be True to run permutation test.')\n",
    "\n",
    "perm_scores_agg = []\n",
    "cv_scores_agg = []\n",
    "pvalues = []\n",
    "model_names = []\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def permutation_test(model):\n",
    "\n",
    "    p = model['params']\n",
    "    p['connectivity__region_to_network'] = aggregation_strategies[p['connectivity__region_to_network']]\n",
    "    pipe.set_params(**p)\n",
    "\n",
    "    _, perm_scores, pvalue = permutation_test_score(pipe, X, y,\n",
    "                                                    scoring='accuracy',\n",
    "                                                    n_permutations=N_PERMUTATIONS,\n",
    "                                                    cv=CV,\n",
    "                                                    n_jobs=-2,\n",
    "                                                    verbose=1)\n",
    "    cv_scores = cross_val_score(pipe, X, y,\n",
    "                                cv=CV,\n",
    "                                scoring='accuracy',\n",
    "                                n_jobs=-2,\n",
    "                                verbose=0)\n",
    "    perm_scores_agg.append(perm_scores)\n",
    "    cv_scores_agg.append(cv_scores)\n",
    "    pvalues.append(pvalue)\n",
    "    model_names.append(model['model_name'])\n",
    "    print(model['model_name'])\n",
    "\n",
    "results.progress_apply(permutation_test, axis=1)\n",
    "\n",
    "# ds_perm_test = xr.Dataset({\n",
    "#     'perm_scores': (('model_name', 'permutation_dim'), perm_scores_agg),\n",
    "#     'cv_scores': (('model_name', 'cv_dim'), cv_scores_agg),\n",
    "#     'pvalue': (('model_name'), pvalues)},\n",
    "#     coords={'model_name': model_names})\n",
    "\n",
    "ds_perm_test = xr.Dataset({\n",
    "    'perm_scores': (('model_name', 'permutation_dim'), perm_scores_agg),\n",
    "    'cv_scores': (('model_name', 'cv_dim'), cv_scores_agg),\n",
    "    'pvalue': (('model_name'), pvalues)},\n",
    "    coords={'model_name': model_names})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# PERMUTATION FEATURE IMPORTANCE (SHUFFLE X)\n",
    "\n",
    "if not ENABLE_PERMUTATION_IMPORTANCE:\n",
    "    raise ValueError('ENABLE_PERMUTATION_IMPORTANCE must be True to run permutation feature importance.')\n",
    "\n",
    "# sort by rank and take top models\n",
    "top_models = pd.DataFrame(grid.cv_results_).sort_values('rank_test_score')[:N_TOP_MODELS].loc[:,'params'].to_list()\n",
    "\n",
    "importances_cv = []\n",
    "\n",
    "for p in (progress_bar := tqdm(top_models)):\n",
    "\n",
    "    pipe.set_params(**p)\n",
    "\n",
    "    model_name = generate_model_name(p)\n",
    "    progress_bar.set_description(model_name)\n",
    "\n",
    "    # get feature names for the connectivity vector\n",
    "    X_conn = pipe[:2].transform(X)\n",
    "    feature_names = pipe[:2].get_feature_names_out()\n",
    "\n",
    "    importances = []\n",
    "\n",
    "    # cross-validated permutation importance\n",
    "    for train, test in tqdm(CV.split(X,y), total=CV.get_n_splits(X,y), desc='CV', leave=False):\n",
    "        pipe.fit(X[train], y[train])\n",
    "\n",
    "        results = permutation_importance(pipe[2:], X_conn[test], y[test],\n",
    "                                        scoring=grid.scoring,\n",
    "                                        n_jobs=-1)\n",
    "        importances.append(results.importances.T)\n",
    "\n",
    "    feature_dim_name = f'{\"_\".join(model_name.split(\"_\")[0:2])}_feature'\n",
    "\n",
    "    importances_cv_dataset = xr.Dataset({\n",
    "        f'{model_name} importances': (('permutation_importance_num', feature_dim_name), np.vstack(importances))},\n",
    "        coords={feature_dim_name: feature_names}\n",
    "    )\n",
    "\n",
    "    importances_cv.append(importances_cv_dataset)\n",
    "    \n",
    "    # sort by mean importance\n",
    "    importances = pd.DataFrame(np.vstack(importances), columns=feature_names)\n",
    "    sorted_columns = importances.mean(axis=0).sort_values(ascending=False).index\n",
    "    importances = importances[sorted_columns]\n",
    "\n",
    "ds_perm_importance = xr.merge(importances_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "outputs": [],
   "source": [
    "# SHAP\n",
    "\n",
    "if not ENABLE_SHAP:\n",
    "    raise ValueError('ENABLE_SHAP must be True to run SHAP analysis.')\n",
    "\n",
    "\n",
    "import shap\n",
    "import logging\n",
    "\n",
    "# turn off shap info-level logs while using progress bars\n",
    "logging.getLogger('shap').setLevel(logging.WARNING)\n",
    "\n",
    "top_models = pd.DataFrame(grid.cv_results_).sort_values('rank_test_score')[:N_TOP_MODELS].loc[:,'params'].to_list()\n",
    "shap_agg = []\n",
    "\n",
    "for p in (progress_bar := tqdm(top_models)):\n",
    "\n",
    "    pipe.set_params(**p)\n",
    "\n",
    "    model_name = generate_model_name(p)\n",
    "    progress_bar.set_description(model_name)\n",
    "\n",
    "    shap_values_cv = []\n",
    "    test_indices = []\n",
    "    y_test_cv = []\n",
    "    y_pred_cv = []\n",
    "\n",
    "    feature_names = pipe[:2].get_feature_names_out()\n",
    "\n",
    "    X_conn = pipe[:2].fit_transform(X, y)\n",
    "\n",
    "    for train, test in tqdm(CV.split(X, y), total=CV.get_n_splits(X, y), desc='CV', leave=False):\n",
    "\n",
    "        shap_model = pipe[2:].fit(X_conn[train], y[train])\n",
    "\n",
    "        y_pred = shap_model.predict(X_conn[test])\n",
    "\n",
    "        test_indices.extend(test)\n",
    "        y_test_cv.append(y[test])\n",
    "        y_pred_cv.append(y_pred)\n",
    "\n",
    "        explainer = shap.Explainer(\n",
    "            shap_model.predict, X_conn[train],\n",
    "            feature_names=feature_names,\n",
    "            # approximate=True,\n",
    "            # model_output='raw',\n",
    "            # feature_perturbation='interventional',\n",
    "        )\n",
    "\n",
    "        shap_values = explainer(X_conn[test], max_evals=2*len(feature_names) + 1)#, check_additivity=True)\n",
    "\n",
    "        shap_values_cv.append(shap_values)\n",
    "\n",
    "    # merge CV SHAPs\n",
    "\n",
    "    # X = subjects.reshape(-1, 1)\n",
    "    # X_test = pd.DataFrame(X[np.hstack(test_indices)], columns=['subject'])\n",
    "    y_test = np.hstack(y_test_cv)\n",
    "    y_pred = np.hstack(y_pred_cv)\n",
    "\n",
    "    shap_values = shap.Explanation(\n",
    "      values = np.vstack([sh.values for sh in shap_values_cv]),\n",
    "      base_values = np.hstack([sh.base_values for sh in shap_values_cv]),\n",
    "      data = np.vstack([sh.data for sh in shap_values_cv]),\n",
    "      feature_names=feature_names,\n",
    "      compute_time=np.sum([sh.compute_time for sh in shap_values_cv]),\n",
    "      output_names=y_encoder.classes_,\n",
    "      output_indexes=y_pred,\n",
    "    )\n",
    "\n",
    "    feature_dim_name = f'{\"_\".join(model_name.split(\"_\")[0:2])}_feature'\n",
    "\n",
    "    shap_ds = xr.Dataset({\n",
    "      f'{model_name} shap': (('shap_dim', feature_dim_name), shap_values.values),\n",
    "      f'{model_name} shap data': (('shap_dim', feature_dim_name), shap_values.data),\n",
    "      f'{model_name} shap y_test': (('shap_dim'), y_encoder.inverse_transform(y_test)),\n",
    "      f'{model_name} shap y_pred': (('shap_dim'), y_encoder.inverse_transform(y_pred)),\n",
    "      },\n",
    "      coords={feature_dim_name: feature_names}\n",
    "    )\n",
    "\n",
    "    shap_agg.append(shap_ds)\n",
    "\n",
    "ds_shap = xr.merge(shap_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# LEARNING CURVE ANALYSIS (HOW DOES TRAIN/TRAIN SIZE IMPACT ACCURACY?)\n",
    "# Note: this only analyze the best model\n",
    "\n",
    "if not ENABLE_LEARNING_CURVE:\n",
    "    raise ValueError('ENABLE_LEARNING_CURVE must be True to run learning curve analysis.')\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(grid.best_estimator_, X, y,\n",
    "                                                        cv=CV,\n",
    "                                                        scoring='accuracy',\n",
    "                                                        n_jobs=-1,\n",
    "                                                        shuffle=True,\n",
    "                                                        train_sizes=np.array([16, 18, 20, 22, 24]))\n",
    "\n",
    "\n",
    "learning_curve_results = pd.DataFrame({\n",
    "    'learning_curve_train_size': train_sizes,\n",
    "    'learning_curve_mean_train_score': train_scores.mean(axis=1),\n",
    "    'learning_curve_mean_test_score': test_scores.mean(axis=1)\n",
    "})\n",
    "\n",
    "learning_curve_results.index.name  = 'learning_curve_num'\n",
    "\n",
    "ds_learning_curve = learning_curve_results.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "32"
    }
   },
   "outputs": [],
   "source": [
    "# %%script echo Skipping...\n",
    "\n",
    "# STORE RESULTS\n",
    "\n",
    "datasets = [\n",
    "    {'X': xr.DataArray(X.flatten(), dims=['subject'])},\n",
    "    {'y': xr.DataArray(y_encoder.inverse_transform(y), dims='subject')},\n",
    "    {'y_classes': y_encoder.classes_},\n",
    "    ds_grid\n",
    "]\n",
    "\n",
    "datasets.append(ds_perm_test) if ENABLE_PERMUTATION_TEST else None\n",
    "datasets.append(ds_perm_importance) if ENABLE_PERMUTATION_IMPORTANCE else None\n",
    "datasets.append(ds_shap) if ENABLE_SHAP else None\n",
    "datasets.append(ds_learning_curve) if ENABLE_LEARNING_CURVE else None\n",
    "\n",
    "results = xr.merge(datasets)\n",
    "\n",
    "with open(OUTPUT_PATH, 'wb') as f:\n",
    "    results.to_netcdf(f, engine='h5netcdf')\n",
    "    results.close()\n",
    "\n",
    "# reload from disk\n",
    "results = xr.open_dataset(OUTPUT_PATH, engine='scipy').load()\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('acnets')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27337377eeae3c8189a7b021b93003f903d6e200a1ea36bbed16bbe086d62899"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
