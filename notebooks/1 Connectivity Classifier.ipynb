{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connectivity Classifier\n",
    "\n",
    "This notebook trains a binary classifier to predict the group of a participant (AVGP or NVGP) based on their connectivity matrices.\n",
    "\n",
    "It implements the following steps:\n",
    "\n",
    "1. Load the data\n",
    "2. Cross-validated classification\n",
    "3. Permutation testing\n",
    "4. Permutation importance\n",
    "5. SHAP\n",
    "\n",
    "\n",
    "## Inputs\n",
    "\n",
    "Connectivity matrices including five connectivity metrics (correlation, partial correlation, tangent, precision, and covariance), five parcellations (DiFuMo64, Dosenbach2010, Gordon2014, Friedman2020, and Seitzman2018), and two aggregation mode (reigon-level and network-level).\n",
    "\n",
    "## Outputs\n",
    "\n",
    "Prediction accuracies on the test set for each combination of connectivity metric, parcellation, and aggregation mode. The results are stored in the following file:\n",
    "  - `models/connectivity_classifier_*.nc5`\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "To run this notebook, you need to have a few packages installed. The easiest way to do this is to use mamba to create a new environment from the `environment.yml` file in the root of this repository:\n",
    "\n",
    "```bash\n",
    "mamba env create -f environment.yml\n",
    "\n",
    "# Then, activate the `acnets` environment:\n",
    "mamba activate acnets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# 0. SETUP\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from src.acnets.pipeline import ConnectivityPipeline, ConnectivityVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import (GridSearchCV, StratifiedShuffleSplit,\n",
    "                                     cross_val_score, learning_curve,\n",
    "                                     permutation_test_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "CV = StratifiedShuffleSplit(n_splits=10, test_size=8)\n",
    "N_PERMUTATIONS = 10\n",
    "N_TOP_MODELS = 10  # number of top models to run permutation test, SHAP, etc.\n",
    "AGGREGATE_NETWORKS = False  # whether aggregate regions into networks or not\n",
    "ENABLE_SHAP = False  # whether to run SHAP analysis or not as it takes a long time\n",
    "\n",
    "MODELS_DIR= Path('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "subjects = ConnectivityPipeline().transform('all').coords['subject'].values\n",
    "groups = [s[:4] for s in subjects]  # AVGP or NVGP\n",
    "\n",
    "X = subjects.reshape(-1, 1)\n",
    "\n",
    "y_encoder = LabelEncoder()\n",
    "y = y_encoder.fit_transform(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# PREPARE OUTPUT DATASET FILE\n",
    "\n",
    "n_splits = CV.get_n_splits()\n",
    "n_folds = int(X.shape[0] / CV.test_size)\n",
    "\n",
    "model_output_name = ('connectivities'\n",
    "                     '_classifier-SVM'\n",
    "                     '_measure-accuracy'\n",
    "                     f'_shap-{\"enabled\" if ENABLE_SHAP else \"disabled\"}'\n",
    "                     f'_agg-{\"network\" if AGGREGATE_NETWORKS else \"region\"}'\n",
    "                     f'_top-{N_TOP_MODELS}'\n",
    "                     f'_cv-{n_splits}x{n_folds}fold.nc5'\n",
    "                     )\n",
    "\n",
    "OUTPUT_PATH = MODELS_DIR / model_output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEFINE PIPELINE\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe  = Pipeline([\n",
    "    ('connectivity', ConnectivityPipeline(atlas='friedman2020', kind='partial correlation', agg_networks=False)),\n",
    "    ('vectorize', ConnectivityVectorizer()),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('zerovar', VarianceThreshold()),\n",
    "    ('select', SelectFromModel(LinearSVC(penalty='l1', dual=False, max_iter=10000),\n",
    "                               max_features=lambda x: min(10, x.shape[1]))),\n",
    "    ('clf', LinearSVC(penalty='l1', dual=False, max_iter=10000))\n",
    "    # ('clf', SVC(kernel='linear', C=1))\n",
    "])\n",
    "\n",
    "# DEBUG (expected to overfit, i.e., score=1)\n",
    "pipe.fit(X, y).score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (mean ± std): 0.55 ± 0.19\n",
      "ConfidenceInterval(low=0.4375, high=0.6625)\n"
     ]
    }
   ],
   "source": [
    "# VERIFY THE MODEL (calculate cross-validated accuracy and bootstrap CI)\n",
    "pipe.set_params(connectivity__atlas='friedman2020',\n",
    "                connectivity__kind='covariance',\n",
    "                connectivity__agg_networks=False)\n",
    "\n",
    "scores = cross_val_score(pipe, X, y,\n",
    "                         cv=CV,\n",
    "                         scoring='accuracy',\n",
    "                         n_jobs=-1)\n",
    "bootstrap_ci = stats.bootstrap(scores.reshape(1,-1), np.mean)\n",
    "\n",
    "print('Test accuracy (mean ± std): {:.2f} ± {:.2f}'.format(scores.mean(), scores.std()))\n",
    "print(bootstrap_ci.confidence_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best estimator: Pipeline(steps=[('connectivity',\n",
      "                 ConnectivityPipeline(atlas='gordon2014_2mm',\n",
      "                                      kind='correlation',\n",
      "                                      agg_networks=False,\n",
      "                                      mock=False,\n",
      "                                      bids_dir='data/julia2018',\n",
      "                                      parcellation_cache_dir='data/julia2018/derivatives/resting_timeseries/')),\n",
      "                ('vectorize', ConnectivityVectorizer()),\n",
      "                ('scale', StandardScaler()), ('zerovar', VarianceThreshold()),\n",
      "                ('select',\n",
      "                 SelectFromModel(estimator=LinearSVC(dual=False, max_iter=10000,\n",
      "                                                     penalty='l1'),\n",
      "                                 max_features=<function <lambda> at 0x7fed75b8f370>)),\n",
      "                ('clf', LinearSVC(dual=False, max_iter=10000, penalty='l1'))]) \n",
      " best score: 0.575\n"
     ]
    }
   ],
   "source": [
    "# RUN PIPELINE ON ALL ATLASES AND METRICS\n",
    "\n",
    "param_grid = {\n",
    "    'connectivity__agg_networks': [AGGREGATE_NETWORKS],\n",
    "    # 'connectivity__atlas': ['gordon2014_2mm', 'dosenbach2010', 'difumo_64_2mm', 'friedman2020'],\n",
    "    'connectivity__atlas': ['gordon2014_2mm'],\n",
    "    'connectivity__kind': ['partial correlation', 'correlation', 'covariance', 'precision'],  # 'tangent' ],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=CV,\n",
    "    verbose=2,\n",
    "    n_jobs=-2,\n",
    "    scoring='accuracy')\n",
    "\n",
    "grid.fit(X, y)\n",
    "clear_output(wait=True)\n",
    "\n",
    "\n",
    "print('best estimator:', grid.best_estimator_, '\\n', 'best score:', grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# STORE RESULTS\n",
    "\n",
    "def generate_model_name(params):\n",
    "    \"\"\"Helper function to generate a model name from the parameters.\"\"\"\n",
    "\n",
    "    atlas_name = params['connectivity__atlas'].replace('_2mm', '').replace('_','')\n",
    "    agg_name = 'networks' if params['connectivity__agg_networks'] else 'regions'\n",
    "    kind_name = params['connectivity__kind'].replace(' ', '-')\n",
    "    name = f'{atlas_name}_{agg_name}_{kind_name}'\n",
    "\n",
    "    return name\n",
    "\n",
    "grid_results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "grid_results['grid_model_name'] = grid_results['params'].apply(generate_model_name)\n",
    "\n",
    "grid_results.set_index('grid_model_name', inplace=True)\n",
    "grid_results.drop(columns=['params'], inplace=True)\n",
    "\n",
    "ds_grid = grid_results.to_xarray()\n",
    "ds_grid['scoring'] = grid.scoring\n",
    "ds_grid['cv_test_size'] = CV.test_size\n",
    "ds_grid['cv_n_splits'] = CV.n_splits\n",
    "ds_grid['n_subjects'] = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a999bf60daa4f4d855172d380a74829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  10 | elapsed:   26.8s remaining:    6.7s\n",
      "[Parallel(n_jobs=-2)]: Done  10 out of  10 | elapsed:   27.3s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  10 | elapsed:   22.8s remaining:    5.7s\n",
      "[Parallel(n_jobs=-2)]: Done  10 out of  10 | elapsed:   24.2s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  10 | elapsed:   36.2s remaining:    9.0s\n",
      "[Parallel(n_jobs=-2)]: Done  10 out of  10 | elapsed:   38.9s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  10 | elapsed:   33.4s remaining:    8.4s\n",
      "[Parallel(n_jobs=-2)]: Done  10 out of  10 | elapsed:   36.4s finished\n"
     ]
    }
   ],
   "source": [
    "# PERMUTATION TEST (SHUFFLE Y)\n",
    "\n",
    "\n",
    "perm_scores_agg = []\n",
    "cv_scores_agg = []\n",
    "pvalues = []\n",
    "model_names = []\n",
    "\n",
    "# sort by rank and take top models\n",
    "top_models = pd.DataFrame(grid.cv_results_).sort_values('rank_test_score')[:N_TOP_MODELS].loc[:,'params'].to_list()\n",
    "\n",
    "for p in (progress_bar := tqdm(top_models)):\n",
    "\n",
    "    pipe.set_params(**p)\n",
    "\n",
    "    model_name = generate_model_name(p)\n",
    "    progress_bar.set_description(model_name)\n",
    "\n",
    "    _, perm_scores, pvalue = permutation_test_score(pipe, X, y,\n",
    "                                                    scoring='accuracy',\n",
    "                                                    n_permutations=N_PERMUTATIONS,\n",
    "                                                    cv=CV,\n",
    "                                                    n_jobs=-2,\n",
    "                                                    verbose=1)\n",
    "\n",
    "    cv_scores = cross_val_score(pipe, X, y,\n",
    "                                cv=CV,\n",
    "                                scoring='accuracy',\n",
    "                                n_jobs=-2,\n",
    "                                verbose=0)\n",
    "\n",
    "    perm_scores_agg.append(perm_scores)\n",
    "    cv_scores_agg.append(cv_scores)\n",
    "    pvalues.append(pvalue)\n",
    "    model_names.append(model_name)\n",
    "\n",
    "ds_perm_test = xr.Dataset({\n",
    "    'perm_scores': (('model_name', 'permutation_dim'), perm_scores_agg),\n",
    "    'cv_scores': (('model_name', 'cv_dim'), cv_scores_agg),\n",
    "    'pvalue': (('model_name'), pvalues)},\n",
    "    coords={'model_name': model_names})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd12468955c443d19bea8fa3f625666a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# get feature names for the connectivity vector\u001b[39;00m\n\u001b[1;32m     13\u001b[0m X_conn \u001b[39m=\u001b[39m pipe[:\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m---> 14\u001b[0m feature_names \u001b[39m=\u001b[39m pipe[:\u001b[39m2\u001b[39;49m]\u001b[39m.\u001b[39;49mget_feature_names_out()\n\u001b[1;32m     16\u001b[0m importances \u001b[39m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m \u001b[39m# cross-validated permutation importance\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/sklearn/pipeline.py:756\u001b[0m, in \u001b[0;36mPipeline.get_feature_names_out\u001b[0;34m(self, input_features)\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(transform, \u001b[39m\"\u001b[39m\u001b[39mget_feature_names_out\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    751\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    752\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mEstimator \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m does not provide get_feature_names_out. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    753\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mDid you mean to call pipeline[:-1].get_feature_names_out\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    754\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m()?\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name)\n\u001b[1;32m    755\u001b[0m         )\n\u001b[0;32m--> 756\u001b[0m     feature_names_out \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mget_feature_names_out(feature_names_out)\n\u001b[1;32m    757\u001b[0m \u001b[39mreturn\u001b[39;00m feature_names_out\n",
      "File \u001b[0;32m~/workspace/acnets/src/acnets/pipeline/connectivity_pipeline.py:103\u001b[0m, in \u001b[0;36mConnectivityPipeline.get_feature_names_out\u001b[0;34m(self, input_features, sep)\u001b[0m\n\u001b[1;32m     97\u001b[0m     input_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(\u001b[39mNone\u001b[39;00m)\u001b[39m.\u001b[39mcoords[\u001b[39m'\u001b[39m\u001b[39mnode\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     99\u001b[0m feature_names \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[1;32m    100\u001b[0m     np\u001b[39m.\u001b[39mzeros((input_features\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], input_features\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])),\n\u001b[1;32m    101\u001b[0m     columns\u001b[39m=\u001b[39minput_features, index\u001b[39m=\u001b[39minput_features)\n\u001b[0;32m--> 103\u001b[0m feature_names \u001b[39m=\u001b[39m feature_names\u001b[39m.\u001b[39;49mstack()\u001b[39m.\u001b[39;49mto_frame()\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    104\u001b[0m     \u001b[39mlambda\u001b[39;49;00m x:\n\u001b[1;32m    105\u001b[0m     sep\u001b[39m.\u001b[39;49mjoin(x\u001b[39m.\u001b[39;49mname) \u001b[39mif\u001b[39;49;00m x\u001b[39m.\u001b[39;49mname[\u001b[39m0\u001b[39;49m] \u001b[39m!=\u001b[39;49m x\u001b[39m.\u001b[39;49mname[\u001b[39m1\u001b[39;49m] \u001b[39melse\u001b[39;49;00m x\u001b[39m.\u001b[39;49mname[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    106\u001b[0m     axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39munstack()\n\u001b[1;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m feature_names\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/pandas/core/frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9557\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9559\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9561\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9566\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9567\u001b[0m )\n\u001b[0;32m-> 9568\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/pandas/core/apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 764\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/pandas/core/apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 891\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    893\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/pandas/core/apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    905\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    906\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 907\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    908\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    909\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    910\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    911\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/workspace/acnets/src/acnets/pipeline/connectivity_pipeline.py:105\u001b[0m, in \u001b[0;36mConnectivityPipeline.get_feature_names_out.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     97\u001b[0m     input_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(\u001b[39mNone\u001b[39;00m)\u001b[39m.\u001b[39mcoords[\u001b[39m'\u001b[39m\u001b[39mnode\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     99\u001b[0m feature_names \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[1;32m    100\u001b[0m     np\u001b[39m.\u001b[39mzeros((input_features\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], input_features\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])),\n\u001b[1;32m    101\u001b[0m     columns\u001b[39m=\u001b[39minput_features, index\u001b[39m=\u001b[39minput_features)\n\u001b[1;32m    103\u001b[0m feature_names \u001b[39m=\u001b[39m feature_names\u001b[39m.\u001b[39mstack()\u001b[39m.\u001b[39mto_frame()\u001b[39m.\u001b[39mapply(\n\u001b[1;32m    104\u001b[0m     \u001b[39mlambda\u001b[39;00m x:\n\u001b[0;32m--> 105\u001b[0m     sep\u001b[39m.\u001b[39;49mjoin(x\u001b[39m.\u001b[39;49mname) \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mname[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m x\u001b[39m.\u001b[39mname[\u001b[39m1\u001b[39m] \u001b[39melse\u001b[39;00m x\u001b[39m.\u001b[39mname[\u001b[39m0\u001b[39m],\n\u001b[1;32m    106\u001b[0m     axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munstack()\n\u001b[1;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m feature_names\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, int found"
     ]
    }
   ],
   "source": [
    "# PERMUTATION FEATURE IMPORTANCE (SHUFFLE X)\n",
    "\n",
    "importances_cv = []\n",
    "\n",
    "for p in (progress_bar := tqdm(top_models)):\n",
    "\n",
    "    pipe.set_params(**p)\n",
    "\n",
    "    model_name = generate_model_name(p)\n",
    "    progress_bar.set_description(model_name)\n",
    "\n",
    "    # get feature names for the connectivity vector\n",
    "    X_conn = pipe[:2].transform(X)\n",
    "    feature_names = pipe[:2].get_feature_names_out()\n",
    "\n",
    "    importances = []\n",
    "\n",
    "    # cross-validated permutation importance\n",
    "    for train, test in tqdm(CV.split(X,y), total=CV.get_n_splits(X,y), desc='CV', leave=False):\n",
    "        pipe.fit(X[train], y[train])\n",
    "\n",
    "        results = permutation_importance(pipe[2:], X_conn[test], y[test],\n",
    "                                        scoring=grid.scoring,\n",
    "                                        n_jobs=-1)\n",
    "        importances.append(results.importances.T)\n",
    "\n",
    "    feature_dim_name = f'{\"_\".join(model_name.split(\"_\")[0:2])}_feature'\n",
    "\n",
    "    importances_cv_dataset = xr.Dataset({\n",
    "        f'{model_name} importances': (('permutation_importance_num', feature_dim_name), np.vstack(importances))},\n",
    "        coords={feature_dim_name: feature_names}\n",
    "    )\n",
    "\n",
    "    importances_cv.append(importances_cv_dataset)\n",
    "    \n",
    "    # sort by mean importance\n",
    "    importances = pd.DataFrame(np.vstack(importances), columns=feature_names)\n",
    "    sorted_columns = importances.mean(axis=0).sort_values(ascending=False).index\n",
    "    importances = importances[sorted_columns]\n",
    "\n",
    "ds_perm_importance = xr.merge(importances_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "outputs": [],
   "source": [
    "# SHAP\n",
    "\n",
    "if not ENABLE_SHAP:\n",
    "    raise ValueError('ENABLE_SHAP must be True to run SHAP analysis')\n",
    "\n",
    "\n",
    "import shap\n",
    "import logging\n",
    "\n",
    "# turn off shap info-level logs while using progress bars\n",
    "logging.getLogger('shap').setLevel(logging.WARNING)\n",
    "\n",
    "shap_agg = []\n",
    "\n",
    "for p in (progress_bar := tqdm(top_models)):\n",
    "\n",
    "    pipe.set_params(**p)\n",
    "\n",
    "    model_name = generate_model_name(p)\n",
    "    progress_bar.set_description(model_name)\n",
    "\n",
    "    shap_values_cv = []\n",
    "    test_indices = []\n",
    "    y_test_cv = []\n",
    "    y_pred_cv = []\n",
    "\n",
    "    feature_names = pipe[:2].get_feature_names_out()\n",
    "\n",
    "    X_conn = pipe[:2].fit_transform(X, y)\n",
    "\n",
    "    for train, test in tqdm(CV.split(X, y), total=CV.get_n_splits(X, y), desc='CV', leave=False):\n",
    "\n",
    "        shap_model = pipe[2:].fit(X_conn[train], y[train])\n",
    "\n",
    "        y_pred = shap_model.predict(X_conn[test])\n",
    "\n",
    "        test_indices.extend(test)\n",
    "        y_test_cv.append(y[test])\n",
    "        y_pred_cv.append(y_pred)\n",
    "\n",
    "        explainer = shap.Explainer(\n",
    "            shap_model.predict, X_conn[train],\n",
    "            feature_names=feature_names,\n",
    "            # approximate=True,\n",
    "            # model_output='raw',\n",
    "            # feature_perturbation='interventional',\n",
    "        )\n",
    "\n",
    "        shap_values = explainer(X_conn[test], max_evals=2*len(feature_names) + 1)#, check_additivity=True)\n",
    "\n",
    "        shap_values_cv.append(shap_values)\n",
    "\n",
    "    # merge CV SHAPs\n",
    "\n",
    "    # X = subjects.reshape(-1, 1)\n",
    "    # X_test = pd.DataFrame(X[np.hstack(test_indices)], columns=['subject'])\n",
    "    y_test = np.hstack(y_test_cv)\n",
    "    y_pred = np.hstack(y_pred_cv)\n",
    "\n",
    "    shap_values = shap.Explanation(\n",
    "      values = np.vstack([sh.values for sh in shap_values_cv]),\n",
    "      base_values = np.hstack([sh.base_values for sh in shap_values_cv]),\n",
    "      data = np.vstack([sh.data for sh in shap_values_cv]),\n",
    "      feature_names=feature_names,\n",
    "      compute_time=np.sum([sh.compute_time for sh in shap_values_cv]),\n",
    "      output_names=y_encoder.classes_,\n",
    "      output_indexes=y_pred,\n",
    "    )\n",
    "\n",
    "    feature_dim_name = f'{\"_\".join(model_name.split(\"_\")[0:2])}_feature'\n",
    "\n",
    "    shap_ds = xr.Dataset({\n",
    "      f'{model_name} shap': (('shap_dim', feature_dim_name), shap_values.values),\n",
    "      f'{model_name} shap data': (('shap_dim', feature_dim_name), shap_values.data),\n",
    "      f'{model_name} shap y_test': (('shap_dim'), y_encoder.inverse_transform(y_test)),\n",
    "      f'{model_name} shap y_pred': (('shap_dim'), y_encoder.inverse_transform(y_pred)),\n",
    "      },\n",
    "      coords={feature_dim_name: feature_names}\n",
    "    )\n",
    "\n",
    "    shap_agg.append(shap_ds)\n",
    "\n",
    "ds_shap = xr.merge(shap_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# LEARNING CURVE ANALYSIS (HOW DOES TRAIN/TRAIN SIZE IMPACT ACCURACY?)\n",
    "# Note: this only analyze the best model\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(grid.best_estimator_, X, y,\n",
    "                                                        cv=CV,\n",
    "                                                        scoring='accuracy',\n",
    "                                                        n_jobs=-1,\n",
    "                                                        shuffle=True,\n",
    "                                                        train_sizes=np.array([16, 18, 20, 22, 24]))\n",
    "\n",
    "\n",
    "learning_curve_results = pd.DataFrame({\n",
    "    'learning_curve_train_size': train_sizes,\n",
    "    'learning_curve_mean_train_score': train_scores.mean(axis=1),\n",
    "    'learning_curve_mean_test_score': test_scores.mean(axis=1)\n",
    "})\n",
    "\n",
    "learning_curve_results.index.name  = 'learning_curve_num'\n",
    "\n",
    "ds_learning_curve = learning_curve_results.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "32"
    }
   },
   "outputs": [],
   "source": [
    "# STORE RESULTS\n",
    "\n",
    "datasets = [\n",
    "    {'X': xr.DataArray(X.flatten(), dims=['subject'])},\n",
    "    {'y': xr.DataArray(y_encoder.inverse_transform(y), dims='subject')},\n",
    "    {'y_classes': y_encoder.classes_},\n",
    "    ds_grid,\n",
    "    ds_learning_curve,\n",
    "    ds_perm_test,\n",
    "    ds_perm_importance]\n",
    "\n",
    "if ENABLE_SHAP:\n",
    "    datasets.append(ds_shap)\n",
    "\n",
    "results = xr.merge(datasets)\n",
    "\n",
    "with open(OUTPUT_PATH, 'wb') as f:\n",
    "    results.to_netcdf(f, engine='h5netcdf')\n",
    "    results.close()\n",
    "\n",
    "# reload from disk\n",
    "results = xr.open_dataset(OUTPUT_PATH, engine='scipy').load()\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('acnets')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27337377eeae3c8189a7b021b93003f903d6e200a1ea36bbed16bbe086d62899"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
