{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connectivity Classifier\n",
    "\n",
    "Steps:\n",
    "1. Load the data\n",
    "2. Fit a SVM + HPO\n",
    "3. Permutation testing\n",
    "4. Permutation Importance\n",
    "5. SHAP\n",
    "\n",
    "## Inputs\n",
    "\n",
    "Connectivity matrices\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- Classification output: Participant's label, either AVGP or NVGP.\n",
    "- Results: `models/1_connectivity_classifier_results.nc`\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "To run this notebook, you need to have a few packages installed:\n",
    "\n",
    "```bash\n",
    "mamba create -n acnets python=3.9 jupyterlab ipykernel \\\n",
    "    matplotlib xarray netcdf4 shap nilearn=0.9.1\n",
    "    # -c rapidsai -c nvidia -y \\\n",
    "    # rapids=22.04 cudatoolkit=11.5 \\\n",
    "\n",
    "\n",
    "mamba activate acnets\n",
    "\n",
    "# pip install statannotations -U\n",
    "\n",
    "# [Optional] Bayesian HBO\n",
    "# pip install \"ray[tune]\" tune-sklearn scikit-optimize\n",
    "```\n",
    "\n",
    "\n",
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# 0. SETUP\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import scipy.stats as st\n",
    "import xarray as xr\n",
    "from python.acnets.pipeline import ConnectivityPipeline, ConnectivityVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import (GridSearchCV, StratifiedShuffleSplit,\n",
    "                                     cross_val_score, learning_curve,\n",
    "                                     permutation_test_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# 0.1. PARAMETERS\n",
    "\n",
    "CV = StratifiedShuffleSplit(n_splits=250, test_size=8)\n",
    "N_PERMUTATIONS = 100\n",
    "\n",
    "OUTPUT_PATH = Path('models/1_connectivity_classifier_results.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# 1. DATA\n",
    "\n",
    "subjects = ConnectivityPipeline().transform('all').coords['subject'].values\n",
    "groups = [s[:4] for s in subjects]  # AVGP or NVGP\n",
    "\n",
    "X = subjects.reshape(-1, 1)\n",
    "\n",
    "y_encoder = LabelEncoder()\n",
    "y = y_encoder.fit_transform(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# 2. PIPELINE\n",
    "\n",
    "pipe  = Pipeline([\n",
    "    ('connectivity', ConnectivityPipeline()),\n",
    "    ('vectorize', ConnectivityVectorizer()),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('zerovar', VarianceThreshold()),\n",
    "    ('select', SelectFromModel(LinearSVC(penalty='l1', dual=False, max_iter=10000))),\n",
    "    ('clf', LinearSVC(penalty='l1', dual=False, max_iter=10000))\n",
    "])\n",
    "\n",
    "# DEBUG\n",
    "# pipe.fit(X, y).score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# 2.1. VERIFY THE MODEL\n",
    "pipe.set_params(connectivity__atlas='dosenbach2010', connectivity__kind='partial correlation')\n",
    "\n",
    "scores = cross_val_score(pipe, X, y,\n",
    "                         cv=CV,\n",
    "                         scoring='accuracy',\n",
    "                         n_jobs=-1)\n",
    "bootstrap_ci = st.bootstrap(scores.reshape(1,-1), np.mean)\n",
    "scores.mean(), scores.std(), bootstrap_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# 3. HPO: GRID SEARCH\n",
    "\n",
    "param_grid = {\n",
    "    'connectivity__atlas': ['gordon2014_2mm', 'dosenbach2010', 'difumo_64_2mm'],\n",
    "    # 'connectivity__atlas': ['seitzman2018'],\n",
    "    'connectivity__kind': ['partial correlation', 'tangent', 'correlation', 'covariance', 'precision'],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=CV,\n",
    "    verbose=1,\n",
    "    scoring='accuracy')\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print('best estimator:', grid.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# 3.1. STORE GRID SEARCH RESULTS\n",
    "\n",
    "#STORE pd.DataFrame(grid.cv_results_).set_index('params')\n",
    "#STORE grid.scoring, grid.cv.test_size,  grid.cv.n_splits, n_subjects\n",
    "grid_results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "grid_results['grid_model_name'] = grid_results['params'].apply(lambda x: ' '.join(x.values()))\n",
    "grid_results.set_index('grid_model_name', inplace=True)\n",
    "grid_results.drop(columns=['params'], inplace=True)\n",
    "\n",
    "ds_grid = grid_results.to_xarray()\n",
    "ds_grid['scoring'] = grid.scoring\n",
    "ds_grid['cv_test_size'] = CV.test_size\n",
    "ds_grid['cv_n_splits'] = CV.n_splits\n",
    "ds_grid['n_subjects'] = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# 4. PERMUTATION TEST (SHUFFLE Y)\n",
    "\n",
    "\n",
    "n_top_models = 3\n",
    "perm_scores_agg = []\n",
    "cv_scores_agg = []\n",
    "pvalues = []\n",
    "model_names = []\n",
    "\n",
    "# sort by rank and take top n_top_models\n",
    "top_models = pd.DataFrame(grid.cv_results_).sort_values('rank_test_score')[:n_top_models].loc[:,'params'].to_list()\n",
    "\n",
    "for p in tqdm(top_models):\n",
    "    model_name = ' '.join(p.values())\n",
    "    \n",
    "    pipe.set_params(**p)\n",
    "\n",
    "    # break if it's a low score\n",
    "\n",
    "    _, perm_scores, pvalue = permutation_test_score(pipe, X, y,\n",
    "                                                    scoring='accuracy',\n",
    "                                                    n_permutations=N_PERMUTATIONS,\n",
    "                                                    cv=4,\n",
    "                                                    n_jobs=-1, verbose=0)\n",
    "\n",
    "    cv_scores = cross_val_score(pipe, X, y,\n",
    "                                cv=CV,\n",
    "                                scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    perm_scores_agg.append(perm_scores)\n",
    "    cv_scores_agg.append(cv_scores)\n",
    "    pvalues.append(pvalue)\n",
    "    model_names.append(model_name)\n",
    "\n",
    "ds_perm = xr.Dataset({'perm_scores': (('model_name', 'permutation_dim'), perm_scores_agg),\n",
    "                 'cv_scores': (('model_name', 'cv_dim'), cv_scores_agg),\n",
    "                 'pvalue': (('model_name'), pvalues)},\n",
    "                 coords={'model_name': model_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# 5. FEATURE IMPORTANCE (SHUFFLE X)\n",
    "\n",
    "importances_agg = []\n",
    "\n",
    "for p in top_models:\n",
    "    model_name = ' '.join(p.values())\n",
    "\n",
    "    pipe.set_params(**p)\n",
    "\n",
    "    X_conn = pipe[:2].transform(X)\n",
    "    feature_names = pipe[:2].get_feature_names_out()\n",
    "\n",
    "    importances = []\n",
    "\n",
    "    for train, test in tqdm(CV.split(X,y), total=CV.get_n_splits(X,y)):\n",
    "        pipe.fit(X[train], y[train])\n",
    "\n",
    "        results = permutation_importance(pipe[2:], X_conn[test], y[test],\n",
    "                                        scoring=grid.scoring,\n",
    "                                        n_jobs=1)\n",
    "        importances.append(results.importances.T)\n",
    "\n",
    "    feature_dim_name = f'{model_name.split(\" \")[0]}_feature'\n",
    "\n",
    "    importances_ds = xr.Dataset({\n",
    "        f'{model_name} importances': (('permutation_importance_num', feature_dim_name), np.vstack(importances))},\n",
    "        coords={feature_dim_name: feature_names}\n",
    "    )\n",
    "\n",
    "    importances_agg.append(importances_ds)\n",
    "    \n",
    "    # sort by mean importance\n",
    "    # importances = pd.DataFrame(np.vstack(importances), columns=feature_names)\n",
    "    # sorted_columns = importances.mean(axis=0).sort_values(ascending=False).index\n",
    "    # importances = importances[sorted_columns]\n",
    "\n",
    "ds_imp = xr.merge(importances_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "outputs": [],
   "source": [
    "# 6. SHAP\n",
    "\n",
    "import shap\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "shap_agg = []\n",
    "\n",
    "for p in top_models:\n",
    "  model_name = ' '.join(p.values())\n",
    "\n",
    "  shap_values_cv = []\n",
    "  test_indices = []\n",
    "  y_test_cv = []\n",
    "  y_pred_cv = []\n",
    "\n",
    "  pipe.set_params(**p)\n",
    "\n",
    "  feature_names = pipe[:2].get_feature_names_out()\n",
    "\n",
    "  n_splits = CV.get_n_splits(X, y)\n",
    "\n",
    "  X_conn = pipe[:2].fit_transform(X, y)\n",
    "\n",
    "  for train, test in tqdm(CV.split(X, y), total=n_splits):\n",
    "\n",
    "      shap_model = pipe[2:].fit(X_conn[train], y[train])\n",
    "\n",
    "      y_pred = shap_model.predict(X_conn[test])\n",
    "\n",
    "      test_indices.extend(test)\n",
    "      y_test_cv.append(y[test])\n",
    "      y_pred_cv.append(y_pred)\n",
    "\n",
    "      explainer = shap.Explainer(\n",
    "          shap_model.predict, X_conn[train],\n",
    "          feature_names=feature_names,\n",
    "          # approximate=True,\n",
    "          # model_output='raw',\n",
    "          # feature_perturbation='interventional',\n",
    "      )\n",
    "\n",
    "      shap_values = explainer(X_conn[test])#, check_additivity=True)\n",
    "\n",
    "      shap_values_cv.append(shap_values)\n",
    "\n",
    "  # merge CV SHAPs\n",
    "\n",
    "  # X = subjects.reshape(-1, 1)\n",
    "  # X_test = pd.DataFrame(X[np.hstack(test_indices)], columns=['subject'])\n",
    "  y_test = np.hstack(y_test_cv)\n",
    "  y_pred = np.hstack(y_pred_cv)\n",
    "\n",
    "  # merge CV SHAPs\n",
    "  shap_values = shap.Explanation(\n",
    "    values = np.vstack([sh.values for sh in shap_values_cv]),\n",
    "    base_values = np.hstack([sh.base_values for sh in shap_values_cv]),\n",
    "    data = np.vstack([sh.data for sh in shap_values_cv]),\n",
    "    feature_names=feature_names,\n",
    "    compute_time=np.sum([sh.compute_time for sh in shap_values_cv]),\n",
    "    output_names=y_encoder.classes_,\n",
    "    output_indexes=y_pred,\n",
    "  )\n",
    "\n",
    "  feature_dim_name = f'{model_name.split(\" \")[0]}_feature'\n",
    "\n",
    "  shap_ds = xr.Dataset({\n",
    "    f'{model_name} shap': (('shap_dim', feature_dim_name), shap_values.values),\n",
    "    f'{model_name} shap data': (('shap_dim', feature_dim_name), shap_values.data),\n",
    "    f'{model_name} shap y_test': (('shap_dim'), y_encoder.inverse_transform(y_test)),\n",
    "    f'{model_name} shap y_pred': (('shap_dim'), y_encoder.inverse_transform(y_pred)),\n",
    "    },\n",
    "    coords={feature_dim_name: feature_names}\n",
    "  )\n",
    "\n",
    "  shap_agg.append(shap_ds)\n",
    "\n",
    "  # STORE y_pred, y_test, shap_values\n",
    "\n",
    "ds_shap = xr.merge(shap_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "outputs": [],
   "source": [
    "# 7. LEARNING CURVE ANALYSIS\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(grid.best_estimator_, X, y,\n",
    "                                                        cv=CV,\n",
    "                                                        scoring='accuracy',\n",
    "                                                        n_jobs=-1,\n",
    "                                                        shuffle=True,\n",
    "                                                        train_sizes=np.array([16, 18, 20, 22, 24]))\n",
    "\n",
    "\n",
    "learning_curve_results = pd.DataFrame({\n",
    "    'learning_curve_train_size': train_sizes,\n",
    "    'learning_curve_mean_train_score': train_scores.mean(axis=1),\n",
    "    'learning_curve_mean_test_score': test_scores.mean(axis=1)\n",
    "})\n",
    "\n",
    "learning_curve_results.index.name  = 'learning_curve_num'\n",
    "\n",
    "ds_learning_curve = learning_curve_results.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "outputs": [],
   "source": [
    "# 8. STORE RESULTS\n",
    "\n",
    "results = xr.merge([\n",
    "    {'X': xr.DataArray(X.flatten(), dims=['subject'])},\n",
    "    {'y': xr.DataArray(y_encoder.inverse_transform(y), dims='subject')},\n",
    "    {'y_classes': y_encoder.classes_},\n",
    "    ds_grid, ds_learning_curve, ds_imp, ds_perm, ds_shap])\n",
    "\n",
    "with open(OUTPUT_PATH, 'wb') as f:\n",
    "    results.to_netcdf(f, engine='scipy')\n",
    "    results.close()\n",
    "\n",
    "results = xr.open_dataset(OUTPUT_PATH, engine='scipy').load()\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('acnets')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34f04479ffaeb5c00adb9e28a92647dce776275bf5ee61de72266754f4451f1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
