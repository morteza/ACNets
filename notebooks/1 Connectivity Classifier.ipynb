{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connectivity Classifier\n",
    "\n",
    "This notebook trains a binary classifier to predict the group of a participant (AVGP or NVGP) based on their connectivity matrices.\n",
    "\n",
    "It implements the following steps:\n",
    "\n",
    "1. Load the data\n",
    "2. Cross-validated classification pipeline\n",
    "3. Permutation testing\n",
    "4. Permutation importance\n",
    "5. SHAP\n",
    "6. Learning curve analysis\n",
    "\n",
    "\n",
    "## Inputs\n",
    "\n",
    "Region-level time-series are extracted using several possible parcellation atlases (i.e., DiFuMo64, Dosenbach2010, Gordon2014, Friedman2020, and Seitzman2018), and several aggregation (regions, networks, and randomized network assignment). The timeseries are then used to calculate connectivity matrices including correlation, partial correlation, tangent, precision, and covariance.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "Prediction accuracies on the test set for each combination of connectivity metric, parcellation, and aggregation mode. The results are stored in the following file:\n",
    "  - `models/connectivity_classifier_*.nc5`\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "To run this notebook, you need to have a few packages installed. The easiest way to do this is to use mamba to create a new environment from the `environment.yml` file in the root of this repository:\n",
    "\n",
    "```bash\n",
    "mamba env create -f environment.yml\n",
    "mamba activate acnets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# 0. SETUP\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from src.acnets.pipeline import ConnectivityPipeline, ConnectivityVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import (GridSearchCV, StratifiedShuffleSplit,\n",
    "                                     cross_val_score, learning_curve,\n",
    "                                     permutation_test_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from src.acnets.pipeline import Parcellation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "These parameters can be set in the command line when running the notebook, or in the notebook itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "N_CV_SPLITS = 10                    # number of cross-validation splits\n",
    "N_TEST_SUBJECTS = 8                 # test size for cross-validation (number of subjects)\n",
    "CV = StratifiedShuffleSplit(n_splits=N_CV_SPLITS, test_size=N_TEST_SUBJECTS)\n",
    "\n",
    "N_PERMUTATIONS = 10                 # for permutation test\n",
    "\n",
    "# Analysis flags\n",
    "ENABLE_SHAP = False                     # run SHAP analysis or not as it takes a long time\n",
    "ENABLE_PERMUTATION_TEST = False         # run permutation test or not as it takes a long time\n",
    "ENABLE_LEARNING_CURVE = False           # run learning curve or not as it takes a long time\n",
    "ENABLE_PERMUTATION_IMPORTANCE = False   # run permutation importance or not as it takes a long time\n",
    "\n",
    "MODELS_DIR= Path('models/')              # Directory to save models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Here we load the data from the `data/julia2018/` dataset. These files contain the connectivity matrices for each participant, for each combination of parcellation and connectivity metric. For the reminder of this notebook, we only focus on `dosenbach2010` parcellation atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# DATA PREPARATION\n",
    "parcellation = Parcellation(atlas_name='dosenbach2010').fit()\n",
    "\n",
    "subjects = parcellation.dataset_.coords['subject'].values\n",
    "\n",
    "# extract group labels (AVGP or NVGP) from subject ids (e.g. AVGP-01)\n",
    "subject_labels = [s[:4] for s in subjects]  \n",
    "\n",
    "X = subjects.reshape(-1, 1)  # subject ids, shape: (n_subjects, 1)\n",
    "\n",
    "y_encoder = LabelEncoder()\n",
    "y = y_encoder.fit_transform(subject_labels)     # labels, shape: (n_subjects,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# PREPARE OUTPUT DATASET FILE\n",
    "\n",
    "n_splits = CV.get_n_splits()\n",
    "n_folds = int(X.shape[0] / CV.test_size)\n",
    "\n",
    "model_output_name = ('connectivities'\n",
    "                     '_classifier-SVM'\n",
    "                     '_measure-accuracy'\n",
    "                     f'_cv-{n_splits}x{n_folds}fold.nc5'\n",
    "                     )\n",
    "\n",
    "OUTPUT_PATH = MODELS_DIR / model_output_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Maps\n",
    "\n",
    "We aggregate the region-level time-series using different strategies. The first strategy is use region time-series (no chance). The second strategy is to average the all region-level time-series across each network. The third strategy is to randomly assign each region to a network, and then average the time-series matrices across each network (random network assignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy 1: region to network\n",
    "region_to_network = parcellation.labels_['network'].to_dict()\n",
    "\n",
    "# strategy 2: region to region\n",
    "region_to_region = {k: k for k in region_to_network.keys()}  \n",
    "\n",
    "# convert them to dataframes\n",
    "region_to_network = pd.DataFrame.from_dict(region_to_network, orient='index',columns=['group'])\n",
    "region_to_region = pd.DataFrame.from_dict(region_to_region, orient='index', columns=['group'])\n",
    "\n",
    "# strategy 3: random network assignment\n",
    "region_to_random_network = region_to_network.copy().rename(columns={'group': 'original_group'})\n",
    "region_to_random_network['group'] = region_to_random_network['original_group'].sample(frac=1).values\n",
    "\n",
    "aggregation_strategies = {\n",
    "    'region': region_to_region,\n",
    "    'network': region_to_network,\n",
    "    'random_network': region_to_random_network\n",
    "}\n",
    "\n",
    "# set names for aggregation strategies; this will be handy later\n",
    "for k, v in aggregation_strategies.items():\n",
    "    v.name = k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "The pipeline is composed of the following steps:\n",
    "\n",
    "1. Extract connectivity matrices from the data\n",
    "2. Vectorize the connectivity matrices\n",
    "3. Scale the connectivity matrices\n",
    "4. Remove zero-variance features\n",
    "5. Select the top 32 features based on the coefficient of a SVM classifier\n",
    "6. SVM binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] overfit accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "# DEFINE PIPELINE\n",
    "\n",
    "pipe  = Pipeline([\n",
    "    ('connectivity', ConnectivityPipeline(kind='partial correlation')),\n",
    "    ('vectorize', ConnectivityVectorizer()),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('zerovar', VarianceThreshold()),\n",
    "    ('select', SelectFromModel(LinearSVC(penalty='l1', dual=False, max_iter=10000),\n",
    "                               max_features=lambda x: min(10, x.shape[1]))),\n",
    "    ('clf', LinearSVC(penalty='l1', dual=False, max_iter=10000))\n",
    "    # ('clf', SVC(kernel='linear', C=1))\n",
    "])\n",
    "\n",
    "# DEBUG (expected to overfit, i.e., score=1)\n",
    "overfit_score = pipe.fit(X, y).score(X, y)\n",
    "print(f'[DEBUG] overfit accuracy: {overfit_score:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the pipeline\n",
    "\n",
    "Here we verify that the pipeline works by running it on all aggregation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timeseries=None, connectivity=None]\n",
      "Test accuracy (mean ± std): 0.42 ± 0.22\n",
      "ConfidenceInterval(low=0.2931273675484434, high=0.5625) \n",
      "\n",
      "[timeseries=network, connectivity=None]\n",
      "Test accuracy (mean ± std): 0.78 ± 0.17\n",
      "ConfidenceInterval(low=0.6375, high=0.85) \n",
      "\n",
      "[timeseries=random_network, connectivity=None]\n",
      "Test accuracy (mean ± std): 0.47 ± 0.12\n",
      "ConfidenceInterval(low=0.4125, high=0.575) \n",
      "\n",
      "[timeseries=None, connectivity=network]\n",
      "Test accuracy (mean ± std): 0.45 ± 0.13\n",
      "ConfidenceInterval(low=0.3875, high=0.55) \n",
      "\n",
      "[timeseries=None, connectivity=random_network]\n",
      "Test accuracy (mean ± std): 0.53 ± 0.15\n",
      "ConfidenceInterval(low=0.425, high=0.6125) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST VARIOUS AGGREGATIONS (calculate cross-validated accuracy and bootstrap CI)\n",
    "\n",
    "for timeseries_aggregation, connectivity_aggregation in [\n",
    "    (None, None),                # no aggregation (regions)\n",
    "    ('network', None),           # time-series aggregation region->network\n",
    "    ('random_network', None),    # time-series aggregation region->random_network\n",
    "    (None, 'network'),           # connectivity matrix aggregation region->network\n",
    "    (None, 'random_network'),    # connectivity matrix aggregation region->random_network\n",
    "    ]:\n",
    "\n",
    "    pipe.set_params(connectivity__atlas='dosenbach2010',\n",
    "                    connectivity__kind='partial correlation',\n",
    "                    connectivity__timeseries_aggregation=timeseries_aggregation,\n",
    "                    connectivity__connectivity_aggregation=connectivity_aggregation)\n",
    "\n",
    "    scores = cross_val_score(pipe, X, y,\n",
    "                            cv=CV,\n",
    "                            scoring='accuracy',\n",
    "                            n_jobs=-1)\n",
    "    bootstrap_ci = stats.bootstrap(scores.reshape(1,-1), np.mean)\n",
    "\n",
    "    print(f'[timeseries={timeseries_aggregation}, connectivity={connectivity_aggregation}]')\n",
    "    print('Test accuracy (mean ± std): {:.2f} ± {:.2f}'.format(scores.mean(), scores.std()))\n",
    "    print(bootstrap_ci.confidence_interval, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best estimator: Pipeline(steps=[('connectivity',\n",
      "                 ConnectivityPipeline(atlas='dosenbach2010',\n",
      "                                      kind='partial correlation',\n",
      "                                      timeseries_aggregation='network',\n",
      "                                      connectivity_aggregation=None,\n",
      "                                      bids_dir='data/julia2018',\n",
      "                                      parcellation_cache_dir='data/julia2018/derivatives/resting_timeseries/')),\n",
      "                ('vectorize', ConnectivityVectorizer()),\n",
      "                ('scale', StandardScaler()), ('zerovar', VarianceThreshold()),\n",
      "                ('select',\n",
      "                 SelectFromModel(estimator=LinearSVC(dual=False, max_iter=10000,\n",
      "                                                     penalty='l1'),\n",
      "                                 max_features=<function <lambda> at 0x7fd14b5dc5e0>)),\n",
      "                ('clf', LinearSVC(dual=False, max_iter=10000, penalty='l1'))]) \n",
      " best score: 0.725\n"
     ]
    }
   ],
   "source": [
    "# RUN PIPELINE ON ALL METRICS\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        # Only connectivity matrix aggregation\n",
    "        'connectivity__timeseries_aggregation': [None],\n",
    "        'connectivity__connectivity_aggregation': [None, 'network', 'random_network'],\n",
    "        'connectivity__atlas': ['dosenbach2010'],  # choices: ['dosenbach2010', 'gordon2014_2mm', 'difumo_64_2mm'],\n",
    "        'connectivity__kind': ['partial correlation', 'correlation', 'covariance', 'precision', 'tangent'],        \n",
    "    },\n",
    "    {\n",
    "        # only time-series aggregation\n",
    "        'connectivity__timeseries_aggregation': ['network', 'random_network'],\n",
    "        'connectivity__connectivity_aggregation': [None],\n",
    "        'connectivity__atlas': ['dosenbach2010'],\n",
    "        'connectivity__kind': ['partial correlation', 'correlation', 'covariance', 'precision', 'tangent'],\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=CV,\n",
    "    verbose=2,\n",
    "    n_jobs=-2,\n",
    "    scoring='accuracy')\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "clear_output(wait=True)\n",
    "print('best estimator:', grid.best_estimator_, '\\n', 'best score:', grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO rewrite the rest of the code to support the new aggregation strategy\n",
    "\n",
    "def get_model_name(params):\n",
    "    \"\"\"Helper function to generate a model name from the parameters.\"\"\"\n",
    "\n",
    "    atlas = params['connectivity__atlas']\n",
    "    kind = params['connectivity__kind'].replace(' ', '')\n",
    "    ts_agg = params['connectivity__timeseries_aggregation'] or 'region'\n",
    "    conn_agg = params['connectivity__connectivity_aggregation'] or 'none'\n",
    "    name = f'{atlas}_kind-{kind}_tsagg-{ts_agg}_connagg-{conn_agg}'\n",
    "\n",
    "    return name\n",
    "\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results['model_name'] = results['params'].apply(get_model_name)\n",
    "\n",
    "results = results.sort_values(by='mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b76f61cb4a74827b7608dd39968f0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  10 | elapsed:    8.1s remaining:    2.0s\n",
      "[Parallel(n_jobs=-2)]: Done  10 out of  10 | elapsed:    8.9s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  10 | elapsed:    8.6s remaining:    2.1s\n",
      "[Parallel(n_jobs=-2)]: Done  10 out of  10 | elapsed:   10.0s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  10 | elapsed:    7.7s remaining:    1.9s\n",
      "[Parallel(n_jobs=-2)]: Done  10 out of  10 | elapsed:    8.8s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  10 | elapsed:    8.3s remaining:    2.1s\n",
      "[Parallel(n_jobs=-2)]: Done  10 out of  10 | elapsed:    9.0s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "/home/morteza/micromamba/envs/acnets/lib/python3.10/site-packages/sklearn/feature_selection/_base.py:96: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  10 | elapsed:    5.8s remaining:    1.5s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(24, 0)) while a minimum of 1 is required by LinearSVC.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/morteza/micromamba/envs/acnets/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 428, in _process_worker\n    r = call_item()\n  File \"/home/morteza/micromamba/envs/acnets/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/morteza/micromamba/envs/acnets/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/morteza/micromamba/envs/acnets/lib/python3.10/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/morteza/micromamba/envs/acnets/lib/python3.10/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/home/morteza/micromamba/envs/acnets/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n  File \"/home/morteza/micromamba/envs/acnets/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 1351, in _permutation_test_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/morteza/micromamba/envs/acnets/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/home/morteza/micromamba/envs/acnets/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 263, in fit\n    X, y = self._validate_data(\n  File \"/home/morteza/micromamba/envs/acnets/lib/python3.10/site-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/morteza/micromamba/envs/acnets/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/home/morteza/micromamba/envs/acnets/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 940, in check_array\n    raise ValueError(\nValueError: Found array with 0 feature(s) (shape=(24, 0)) while a minimum of 1 is required by LinearSVC.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     pvalues\u001b[39m.\u001b[39mappend(pvalue)\n\u001b[1;32m     32\u001b[0m     model_names\u001b[39m.\u001b[39mappend(model[\u001b[39m'\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 34\u001b[0m results\u001b[39m.\u001b[39;49mprogress_apply(permutation_test, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     36\u001b[0m results_perm_test \u001b[39m=\u001b[39m xr\u001b[39m.\u001b[39mDataset({\n\u001b[1;32m     37\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mperm_scores\u001b[39m\u001b[39m'\u001b[39m: ((\u001b[39m'\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpermutation_dim\u001b[39m\u001b[39m'\u001b[39m), perm_scores),\n\u001b[1;32m     38\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcv_scores\u001b[39m\u001b[39m'\u001b[39m: ((\u001b[39m'\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcv_dim\u001b[39m\u001b[39m'\u001b[39m), cv_scores),\n\u001b[1;32m     39\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mpvalue\u001b[39m\u001b[39m'\u001b[39m: ((\u001b[39m'\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m'\u001b[39m), pvalues)},\n\u001b[1;32m     40\u001b[0m     coords\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m'\u001b[39m: model_names})\n\u001b[1;32m     42\u001b[0m results_perm_test\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/tqdm/std.py:805\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[39m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(df, df_function)(wrapper, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    806\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     t\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/pandas/core/frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9412\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9414\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9415\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9416\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9421\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9422\u001b[0m )\n\u001b[0;32m-> 9423\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/pandas/core/apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    676\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/pandas/core/apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 798\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    800\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/pandas/core/apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    812\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    813\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    815\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    816\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    817\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/tqdm/std.py:800\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    795\u001b[0m     \u001b[39m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     \u001b[39m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    797\u001b[0m     \u001b[39m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    798\u001b[0m     \u001b[39m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    799\u001b[0m     t\u001b[39m.\u001b[39mupdate(n\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39mtotal \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mn \u001b[39m<\u001b[39m t\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[0;32m--> 800\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36mpermutation_test\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     15\u001b[0m model_params \u001b[39m=\u001b[39m model[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m pipe\u001b[39m.\u001b[39mset_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_params)\n\u001b[0;32m---> 18\u001b[0m _, perm_score, pvalue \u001b[39m=\u001b[39m permutation_test_score(pipe, X, y,\n\u001b[1;32m     19\u001b[0m                                                scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     20\u001b[0m                                                n_permutations\u001b[39m=\u001b[39;49mN_PERMUTATIONS,\n\u001b[1;32m     21\u001b[0m                                                cv\u001b[39m=\u001b[39;49mCV,\n\u001b[1;32m     22\u001b[0m                                                n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     23\u001b[0m                                                verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     24\u001b[0m cv_score \u001b[39m=\u001b[39m cross_val_score(pipe, X, y,\n\u001b[1;32m     25\u001b[0m                             cv\u001b[39m=\u001b[39mCV,\n\u001b[1;32m     26\u001b[0m                             scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     27\u001b[0m                             n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     28\u001b[0m                             verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     29\u001b[0m perm_scores\u001b[39m.\u001b[39mappend(perm_score)\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:1325\u001b[0m, in \u001b[0;36mpermutation_test_score\u001b[0;34m(estimator, X, y, groups, cv, n_permutations, n_jobs, random_state, verbose, scoring, fit_params)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m score \u001b[39m=\u001b[39m _permutation_test_score(\n\u001b[1;32m   1323\u001b[0m     clone(estimator), X, y, groups, cv, scorer, fit_params\u001b[39m=\u001b[39mfit_params\n\u001b[1;32m   1324\u001b[0m )\n\u001b[0;32m-> 1325\u001b[0m permutation_scores \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49mverbose)(\n\u001b[1;32m   1326\u001b[0m     delayed(_permutation_test_score)(\n\u001b[1;32m   1327\u001b[0m         clone(estimator),\n\u001b[1;32m   1328\u001b[0m         X,\n\u001b[1;32m   1329\u001b[0m         _shuffle(y, groups, random_state),\n\u001b[1;32m   1330\u001b[0m         groups,\n\u001b[1;32m   1331\u001b[0m         cv,\n\u001b[1;32m   1332\u001b[0m         scorer,\n\u001b[1;32m   1333\u001b[0m         fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m   1334\u001b[0m     )\n\u001b[1;32m   1335\u001b[0m     \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(n_permutations)\n\u001b[1;32m   1336\u001b[0m )\n\u001b[1;32m   1337\u001b[0m permutation_scores \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(permutation_scores)\n\u001b[1;32m   1338\u001b[0m pvalue \u001b[39m=\u001b[39m (np\u001b[39m.\u001b[39msum(permutation_scores \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m score) \u001b[39m+\u001b[39m \u001b[39m1.0\u001b[39m) \u001b[39m/\u001b[39m (n_permutations \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(24, 0)) while a minimum of 1 is required by LinearSVC."
     ]
    }
   ],
   "source": [
    "# PERMUTATION TEST (SHUFFLE Y) + CV Scores for all models\n",
    "\n",
    "# if not ENABLE_PERMUTATION_TEST:\n",
    "#     raise ValueError('ENABLE_PERMUTATION_TEST must be True to run permutation test.')\n",
    "\n",
    "perm_scores = []\n",
    "cv_scores = []\n",
    "pvalues = []\n",
    "model_names = []\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def permutation_test(model):\n",
    "\n",
    "    model_params = model['params']\n",
    "    pipe.set_params(**model_params)\n",
    "\n",
    "    _, perm_score, pvalue = permutation_test_score(pipe, X, y,\n",
    "                                                   scoring='accuracy',\n",
    "                                                   n_permutations=N_PERMUTATIONS,\n",
    "                                                   cv=CV,\n",
    "                                                   n_jobs=-2,\n",
    "                                                   verbose=1)\n",
    "    cv_score = cross_val_score(pipe, X, y,\n",
    "                                cv=CV,\n",
    "                                scoring='accuracy',\n",
    "                                n_jobs=-2,\n",
    "                                verbose=0)\n",
    "    perm_scores.append(perm_score)\n",
    "    cv_scores.append(cv_score)\n",
    "    pvalues.append(pvalue)\n",
    "    model_names.append(model['model_name'])\n",
    "\n",
    "results.progress_apply(permutation_test, axis=1)\n",
    "\n",
    "results_perm_test = xr.Dataset({\n",
    "    'perm_scores': (('model_name', 'permutation_dim'), perm_scores),\n",
    "    'cv_scores': (('model_name', 'cv_dim'), cv_scores),\n",
    "    'pvalue': (('model_name'), pvalues)},\n",
    "    coords={'model_name': model_names})\n",
    "\n",
    "results_perm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae28167d9564b61baef9546be756ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eeecee59f4745779a6412843f59df37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04c7f3776ea4653820d7cd8e010fe05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08706faf4e14b0789117a6b6d69eb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb63d84b02344af9bedba81497a2de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af13739e9174a7895468af092ee806a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3c37440c834900b6f95c17e5f44b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6636c811f7a24a139a1f2814f1e6e0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d8adc491494f69a8030b094286389c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a77dc83bd44953a4bc28606312bef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c0e0c43cf1419daac9e454e686e86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df10ddb005e4b73b50d444e55f92012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334c5e0ed57843e7a68ef9f151cbf445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m tqdm(CV\u001b[39m.\u001b[39msplit(X,y), total\u001b[39m=\u001b[39mCV\u001b[39m.\u001b[39mget_n_splits(X,y), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCV\u001b[39m\u001b[39m'\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     26\u001b[0m     pipe\u001b[39m.\u001b[39mfit(X[train], y[train])\n\u001b[0;32m---> 28\u001b[0m     results \u001b[39m=\u001b[39m permutation_importance(pipe[\u001b[39m2\u001b[39;49m:], X_conn[test], y[test],\n\u001b[1;32m     29\u001b[0m                                     scoring\u001b[39m=\u001b[39;49mgrid\u001b[39m.\u001b[39;49mscoring,\n\u001b[1;32m     30\u001b[0m                                     n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     31\u001b[0m     importances\u001b[39m.\u001b[39mappend(results\u001b[39m.\u001b[39mimportances\u001b[39m.\u001b[39mT)\n\u001b[1;32m     33\u001b[0m feature_dim_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(model_name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m:\u001b[39m2\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m_feature\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/sklearn/inspection/_permutation_importance.py:258\u001b[0m, in \u001b[0;36mpermutation_importance\u001b[0;34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state, sample_weight, max_samples)\u001b[0m\n\u001b[1;32m    254\u001b[0m     scorer \u001b[39m=\u001b[39m _MultimetricScorer(scorers\u001b[39m=\u001b[39mscorers_dict)\n\u001b[1;32m    256\u001b[0m baseline_score \u001b[39m=\u001b[39m _weights_scorer(scorer, estimator, X, y, sample_weight)\n\u001b[0;32m--> 258\u001b[0m scores \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49mn_jobs)(\n\u001b[1;32m    259\u001b[0m     delayed(_calculate_permutation_scores)(\n\u001b[1;32m    260\u001b[0m         estimator,\n\u001b[1;32m    261\u001b[0m         X,\n\u001b[1;32m    262\u001b[0m         y,\n\u001b[1;32m    263\u001b[0m         sample_weight,\n\u001b[1;32m    264\u001b[0m         col_idx,\n\u001b[1;32m    265\u001b[0m         random_seed,\n\u001b[1;32m    266\u001b[0m         n_repeats,\n\u001b[1;32m    267\u001b[0m         scorer,\n\u001b[1;32m    268\u001b[0m         max_samples,\n\u001b[1;32m    269\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[39mfor\u001b[39;49;00m col_idx \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(X\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m])\n\u001b[1;32m    271\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(baseline_score, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    274\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    275\u001b[0m         name: _create_importances_bunch(\n\u001b[1;32m    276\u001b[0m             baseline_score[name],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m baseline_score\n\u001b[1;32m    281\u001b[0m     }\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# PERMUTATION FEATURE IMPORTANCE (SHUFFLE X)\n",
    "\n",
    "# if not ENABLE_PERMUTATION_IMPORTANCE:\n",
    "#     raise RuntimeError('ENABLE_PERMUTATION_IMPORTANCE must be True to run permutation feature importance.')\n",
    "\n",
    "# sort by rank and take top models\n",
    "top_models = pd.DataFrame(grid.cv_results_).sort_values('rank_test_score').loc[:,'params'].to_list()\n",
    "\n",
    "importances_cv = []\n",
    "\n",
    "for p in (progress_bar := tqdm(top_models)):\n",
    "\n",
    "    model_name = get_model_name(p)\n",
    "    progress_bar.set_description(model_name)\n",
    "\n",
    "    pipe.set_params(**p)\n",
    "\n",
    "    # get feature names for the connectivity vector\n",
    "    X_conn = pipe[:2].transform(X)\n",
    "    feature_names = pipe[:2].get_feature_names_out()\n",
    "\n",
    "    importances = []\n",
    "\n",
    "    # cross-validated permutation importance\n",
    "    for train, test in tqdm(CV.split(X,y), total=CV.get_n_splits(X,y), desc='CV', leave=False):\n",
    "        pipe.fit(X[train], y[train])\n",
    "\n",
    "        results = permutation_importance(pipe[2:], X_conn[test], y[test],\n",
    "                                        scoring=grid.scoring,\n",
    "                                        n_jobs=-1)\n",
    "        importances.append(results.importances.T)\n",
    "\n",
    "    feature_dim_name = f'{\"_\".join(model_name.split(\"_\")[0:2])}_feature'\n",
    "\n",
    "    importances_cv_dataset = xr.Dataset({\n",
    "        f'{model_name} importances': (('permutation_importance_num', feature_dim_name), np.vstack(importances))},\n",
    "        coords={feature_dim_name: feature_names}\n",
    "    )\n",
    "\n",
    "    importances_cv.append(importances_cv_dataset)\n",
    "    \n",
    "    # sort by mean importance\n",
    "    importances = pd.DataFrame(np.vstack(importances), columns=feature_names)\n",
    "    sorted_columns = importances.mean(axis=0).sort_values(ascending=False).index\n",
    "    importances = importances[sorted_columns]\n",
    "\n",
    "results_perm_importance = xr.merge(importances_cv)\n",
    "\n",
    "results_perm_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "outputs": [],
   "source": [
    "# SHAP\n",
    "\n",
    "if not ENABLE_SHAP:\n",
    "    raise ValueError('ENABLE_SHAP must be True to run SHAP analysis.')\n",
    "\n",
    "\n",
    "import shap\n",
    "import logging\n",
    "\n",
    "# turn off shap info-level logs while using progress bars\n",
    "logging.getLogger('shap').setLevel(logging.WARNING)\n",
    "\n",
    "top_models = pd.DataFrame(grid.cv_results_).sort_values('rank_test_score')[:N_TOP_MODELS].loc[:,'params'].to_list()\n",
    "shap_agg = []\n",
    "\n",
    "for p in (progress_bar := tqdm(top_models)):\n",
    "\n",
    "    pipe.set_params(**p)\n",
    "\n",
    "    model_name = generate_model_name(p)\n",
    "    progress_bar.set_description(model_name)\n",
    "\n",
    "    shap_values_cv = []\n",
    "    test_indices = []\n",
    "    y_test_cv = []\n",
    "    y_pred_cv = []\n",
    "\n",
    "    feature_names = pipe[:2].get_feature_names_out()\n",
    "\n",
    "    X_conn = pipe[:2].fit_transform(X, y)\n",
    "\n",
    "    for train, test in tqdm(CV.split(X, y), total=CV.get_n_splits(X, y), desc='CV', leave=False):\n",
    "\n",
    "        shap_model = pipe[2:].fit(X_conn[train], y[train])\n",
    "\n",
    "        y_pred = shap_model.predict(X_conn[test])\n",
    "\n",
    "        test_indices.extend(test)\n",
    "        y_test_cv.append(y[test])\n",
    "        y_pred_cv.append(y_pred)\n",
    "\n",
    "        explainer = shap.Explainer(\n",
    "            shap_model.predict, X_conn[train],\n",
    "            feature_names=feature_names,\n",
    "            # approximate=True,\n",
    "            # model_output='raw',\n",
    "            # feature_perturbation='interventional',\n",
    "        )\n",
    "\n",
    "        shap_values = explainer(X_conn[test], max_evals=2*len(feature_names) + 1)#, check_additivity=True)\n",
    "\n",
    "        shap_values_cv.append(shap_values)\n",
    "\n",
    "    # merge CV SHAPs\n",
    "\n",
    "    # X = subjects.reshape(-1, 1)\n",
    "    # X_test = pd.DataFrame(X[np.hstack(test_indices)], columns=['subject'])\n",
    "    y_test = np.hstack(y_test_cv)\n",
    "    y_pred = np.hstack(y_pred_cv)\n",
    "\n",
    "    shap_values = shap.Explanation(\n",
    "      values = np.vstack([sh.values for sh in shap_values_cv]),\n",
    "      base_values = np.hstack([sh.base_values for sh in shap_values_cv]),\n",
    "      data = np.vstack([sh.data for sh in shap_values_cv]),\n",
    "      feature_names=feature_names,\n",
    "      compute_time=np.sum([sh.compute_time for sh in shap_values_cv]),\n",
    "      output_names=y_encoder.classes_,\n",
    "      output_indexes=y_pred,\n",
    "    )\n",
    "\n",
    "    feature_dim_name = f'{\"_\".join(model_name.split(\"_\")[0:2])}_feature'\n",
    "\n",
    "    shap_ds = xr.Dataset({\n",
    "      f'{model_name} shap': (('shap_dim', feature_dim_name), shap_values.values),\n",
    "      f'{model_name} shap data': (('shap_dim', feature_dim_name), shap_values.data),\n",
    "      f'{model_name} shap y_test': (('shap_dim'), y_encoder.inverse_transform(y_test)),\n",
    "      f'{model_name} shap y_pred': (('shap_dim'), y_encoder.inverse_transform(y_pred)),\n",
    "      },\n",
    "      coords={feature_dim_name: feature_names}\n",
    "    )\n",
    "\n",
    "    shap_agg.append(shap_ds)\n",
    "\n",
    "ds_shap = xr.merge(shap_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# LEARNING CURVE ANALYSIS (HOW DOES TRAIN/TRAIN SIZE IMPACT ACCURACY?)\n",
    "# Note: this only analyze the best model\n",
    "\n",
    "if not ENABLE_LEARNING_CURVE:\n",
    "    raise ValueError('ENABLE_LEARNING_CURVE must be True to run learning curve analysis.')\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(grid.best_estimator_, X, y,\n",
    "                                                        cv=CV,\n",
    "                                                        scoring='accuracy',\n",
    "                                                        n_jobs=-1,\n",
    "                                                        shuffle=True,\n",
    "                                                        train_sizes=np.array([16, 18, 20, 22, 24]))\n",
    "\n",
    "\n",
    "learning_curve_results = pd.DataFrame({\n",
    "    'learning_curve_train_size': train_sizes,\n",
    "    'learning_curve_mean_train_score': train_scores.mean(axis=1),\n",
    "    'learning_curve_mean_test_score': test_scores.mean(axis=1)\n",
    "})\n",
    "\n",
    "learning_curve_results.index.name  = 'learning_curve_num'\n",
    "\n",
    "ds_learning_curve = learning_curve_results.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "32"
    }
   },
   "outputs": [],
   "source": [
    "# %%script echo Skipping...\n",
    "\n",
    "# STORE RESULTS\n",
    "\n",
    "datasets = [\n",
    "    {'X': xr.DataArray(X.flatten(), dims=['subject'])},\n",
    "    {'y': xr.DataArray(y_encoder.inverse_transform(y), dims='subject')},\n",
    "    {'y_classes': y_encoder.classes_},\n",
    "    ds_grid\n",
    "]\n",
    "\n",
    "datasets.append(ds_perm_test) if ENABLE_PERMUTATION_TEST else None\n",
    "datasets.append(ds_perm_importance) if ENABLE_PERMUTATION_IMPORTANCE else None\n",
    "datasets.append(ds_shap) if ENABLE_SHAP else None\n",
    "datasets.append(ds_learning_curve) if ENABLE_LEARNING_CURVE else None\n",
    "\n",
    "results = xr.merge(datasets)\n",
    "\n",
    "with open(OUTPUT_PATH, 'wb') as f:\n",
    "    results.to_netcdf(f, engine='h5netcdf')\n",
    "    results.close()\n",
    "\n",
    "# reload from disk\n",
    "results = xr.open_dataset(OUTPUT_PATH, engine='scipy').load()\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('acnets')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27337377eeae3c8189a7b021b93003f903d6e200a1ea36bbed16bbe086d62899"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
