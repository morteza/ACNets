{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "from src.acnets.pipeline import Parcellation\n",
    "from src.acnets.pipeline import TimeseriesAggregator, ConnectivityExtractor\n",
    "from src.acnets.pipeline import ConnectivityAggregator, ConnectivityVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_union\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tqdm import tqdm\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H1\n",
    "\n",
    "class ExtractH1Features(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, dataset, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataset):\n",
    "        self.feature_names = dataset['timeseries'].coords['region'].values\n",
    "\n",
    "        features = dataset['timeseries'].mean('timepoint').values\n",
    "\n",
    "        return features\n",
    "\n",
    "    def get_feature_names_out(self, input_features):\n",
    "        return self.feature_names\n",
    "\n",
    "pipelines['h1'] = Pipeline([\n",
    "    ('extract_features', ExtractH1Features()),\n",
    "    # TODO normalize timeseries\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H2\n",
    "# within-network connectivity\n",
    "\n",
    "class ExtractH2Features(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, dataset, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataset):\n",
    "        node_type = dataset['connectivity'].dims[-1]\n",
    "        self.feature_names = dataset['connectivity'].coords[node_type].values.tolist()\n",
    "\n",
    "        conn_vec = np.array([np.diag(conn)\n",
    "                                  for conn in dataset['connectivity'].values])\n",
    "\n",
    "        return conn_vec\n",
    "\n",
    "    def get_feature_names_out(self, input_features):\n",
    "        return self.feature_names\n",
    "\n",
    "pipelines['h2'] = Pipeline([\n",
    "    ('aggregate_ts', TimeseriesAggregator(strategy=None)),\n",
    "    ('extract_conn', ConnectivityExtractor(kind='partial correlation')),\n",
    "    ('aggregate_conn', ConnectivityAggregator(strategy='network')),\n",
    "    ('extract_features', ExtractH2Features())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H3: between-network connectivity\n",
    "# non-diagonal connectivity between networks (shape: N_networks * N_networks / 2)\n",
    "\n",
    "class ExtractH3Features(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, k=0):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, dataset, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataset):\n",
    "        conns = dataset['connectivity'].values\n",
    "        conn_vectorized = np.array([conn[np.triu_indices(conn.shape[0], k=self.k)]\n",
    "                           for conn in conns])\n",
    "\n",
    "        node_type = dataset['connectivity'].dims[-1][:-4]\n",
    "\n",
    "        self.feature_names = pd.DataFrame(\n",
    "            data=np.zeros((conns.shape[1], conns.shape[2])),\n",
    "            columns=dataset[node_type + '_src'],\n",
    "            index=dataset[node_type + '_dst'])\n",
    "\n",
    "        sep = ' \\N{left right arrow} '\n",
    "        self.feature_names = (self.feature_names\n",
    "                                  .stack().to_frame()\n",
    "                                  .apply(\n",
    "                                      lambda x: sep.join(x.name), axis=1)\n",
    "                                  .unstack()).values\n",
    "        self.feature_names = self.feature_names[np.triu_indices(self.feature_names.shape[0],\n",
    "                                                                k=self.k)].tolist()\n",
    "\n",
    "        return conn_vectorized\n",
    "\n",
    "    def get_feature_names_out(self, input_features):\n",
    "        return self.feature_names\n",
    "\n",
    "pipelines['h3'] = Pipeline([\n",
    "    ('aggregate_ts', TimeseriesAggregator(strategy='network')),\n",
    "    ('extract_conn', ConnectivityExtractor(kind='partial correlation')),\n",
    "    ('extract_features', ExtractH3Features())\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined model\n",
    "\n",
    "# Input/Output\n",
    "parcellation = Parcellation(atlas_name='dosenbach2010').fit()\n",
    "subjects = parcellation.dataset_.coords['subject'].values\n",
    "subject_labels = [s[:4] for s in subjects]  \n",
    "X = subjects.reshape(-1,1)                     # subjects, shape: (n_subjects, 1)\n",
    "y_encoder = LabelEncoder()\n",
    "y = y_encoder.fit_transform(subject_labels)     # labels, shape: (n_subjects,)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model  = Pipeline([\n",
    "    ('parcellation', Parcellation(atlas_name='dosenbach2010')),\n",
    "    ('extract_features', make_union(*pipelines.values())),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('zerovar', VarianceThreshold()),\n",
    "    ('clf', XGBClassifier())\n",
    "    # ('ica', FastICA(n_components=20)),\n",
    "    # ('select', SelectFromModel(RandomForestClassifier(),\n",
    "    #                            max_features=lambda x: min(10, x.shape[1]))),\n",
    "    # ('clf', RandomForestClassifier())\n",
    "    # ('select', SelectFromModel(LinearSVC(penalty='l2', dual=False, max_iter=10000),\n",
    "    #                            max_features=lambda x: min(10, x.shape[1]))),\n",
    "    # ('clf', LinearSVC(penalty='l2', dual=False, max_iter=10000))\n",
    "])\n",
    "\n",
    "# DEBUG (expected to overfit, i.e., score=1)\n",
    "overfit_score = model.fit(X, y).score(X, y)\n",
    "print(f'[DEBUG] overfit accuracy: {overfit_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CV = StratifiedShuffleSplit(n_splits=20, test_size=8)\n",
    "cross_val_score(model, X, y, cv=CV, verbose=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_results = []\n",
    "feature_names = model[:2].get_feature_names_out()\n",
    "\n",
    "permutation_cv = StratifiedShuffleSplit(n_splits=20, test_size=8)\n",
    "\n",
    "for train, test in tqdm(permutation_cv.split(X,y), total=permutation_cv.get_n_splits(X,y)):\n",
    "    model.fit(X[train], y[train])\n",
    "    X_features = model[:2].transform(X)\n",
    "\n",
    "    _results = permutation_importance(model[2:], X_features[test], y[test],\n",
    "                                    scoring='accuracy')\n",
    "    feature_importance_results.append(_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame(\n",
    "    data=np.stack([imp['importances_mean'] for imp in feature_importance_results]),\n",
    "    columns=feature_names).mean().sort_values(ascending=False).to_frame('importance')\n",
    "\n",
    "importances[:10]  # top 10 features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
