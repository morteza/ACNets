{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "We implement three methods for feature importance:\n",
    "- permutation importance\n",
    "- weights from the fitted classifier\n",
    "- Multipass and SHAP\n",
    "- DropCol (custom-made)\n",
    "\n",
    "It either uses weights from the fitted classifier or calculates permutation importance. The latter is more computationally expensive but more reliable for all kinds of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import skexplain\n",
    "from skexplain.common.importance_utils import to_skexplain_importance\n",
    "\n",
    "from src.acnets.pipeline import MultiScaleClassifier, Parcellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUNS = 100      # 10 independent train/test runs\n",
    "TEST_SIZE = .25  # proportion of test subjects out of 32 subjects\n",
    "\n",
    "PARAM_FILE = 'models/multiscale_classifier-XGBClassifier-hpo.json'\n",
    "\n",
    "# load parameters and init model\n",
    "with open(PARAM_FILE, 'r') as f:\n",
    "    model_params = json.load(f)\n",
    "    classifier = XGBClassifier(base_score=.5, objective='binary:logistic')\n",
    "    model = MultiScaleClassifier(classifier=classifier).set_params(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] label mapping: {'AVGP': 0, 'NVGP': 1}\n",
      "[DEBUG] overfit accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Xy\n",
    "subjects = Parcellation(atlas_name='difumo_64_2mm').fit_transform(None).coords['subject'].values\n",
    "X = subjects.reshape(-1,1)                                  # subjects ids, shape: (n_subjects, 1)\n",
    "\n",
    "y_encoder = LabelEncoder()\n",
    "y = y_encoder.fit_transform([s[:4] for s in subjects])      # labels (AVGP=1 or NVGP=1), shape: (n_subjects,)\n",
    "y_mapping = dict(zip(y_encoder.classes_, y_encoder.transform(y_encoder.classes_)))\n",
    "\n",
    "# DEBUG (report label mapping)\n",
    "print('[DEBUG] label mapping:', y_mapping)\n",
    "\n",
    "# DEBUG (expected to overfit, i.e., accuracy is 1)\n",
    "overfit_score = model.fit(X, y).score(X, y)\n",
    "print(f'[DEBUG] overfit accuracy: {overfit_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Feature Importance\n",
    "\n",
    "def get_importance(estimator, X_train, y_train, X_test, y_test, scoring='accuracy', use_coef=False):\n",
    "    \"\"\"Perform permutation importance analysis on a given estimator.\"\"\"\n",
    "\n",
    "    estimator.fit(X_train, y_train)\n",
    "    feature_names = estimator.get_feature_extractor().get_feature_names_out()\n",
    "    selected_feature_names = estimator[2:-1].get_feature_names_out(feature_names)\n",
    "\n",
    "    if use_coef and 'xgb' in estimator.named_steps['clf'].__class__.__name__.lower():\n",
    "        # for xgb classifier we use its booster's get_score() method\n",
    "        booster = estimator.named_steps['clf'].get_booster()\n",
    "        booster.feature_names = selected_feature_names.tolist()\n",
    "        importance = booster.get_score(importance_type='weight', fmap='')\n",
    "    elif use_coef and 'feature_importances_' in dir(estimator.named_steps['clf']):\n",
    "        # for some other classifiers we use their feature_importances_ attribute\n",
    "        importance = estimator.named_steps['clf'].feature_importances_\n",
    "        importance = dict(zip(feature_names, importance))\n",
    "    else:\n",
    "        # generic permutation importance\n",
    "        X_test_features = estimator.get_feature_extractor().transform(X_test)\n",
    "        feature_names = estimator.get_feature_extractor().get_feature_names_out()\n",
    "        importance = permutation_importance(estimator.get_classification_head(),\n",
    "                                            X_test_features,\n",
    "                                            y_test,\n",
    "                                            n_jobs=-1,\n",
    "                                            scoring=scoring)['importances_mean']\n",
    "        importance = dict(zip(feature_names, importance))\n",
    "\n",
    "    importance_df = pd.DataFrame([(k, v)\n",
    "                                  for k, v in importance.items()],\n",
    "                                 columns=['feature', 'importance'])\n",
    "    return importance_df\n",
    "\n",
    "\n",
    "# run permutation importance in parallel\n",
    "importances = Parallel(n_jobs=-1, verbose=2)(\n",
    "    delayed(get_importance)(\n",
    "        estimator = model,\n",
    "        X_train = X[train],\n",
    "        y_train = y[train],\n",
    "        X_test = X[test],\n",
    "        y_test = y[test],\n",
    "        use_coef=False)\n",
    "    for train, test in StratifiedShuffleSplit(n_splits=N_RUNS, test_size=TEST_SIZE).split(X, y)\n",
    ")\n",
    "\n",
    "# convert importance scores to dataframe and sort\n",
    "importances = pd.concat(importances, axis=0).reset_index(drop=True)\n",
    "importances['feature_group'] = importances['feature'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "# sort features by mean importance and select top-20 \n",
    "order = (importances.groupby('feature')[['importance']]\n",
    "                    .mean()\n",
    "                    .sort_values('importance', ascending=False)[:20].index)\n",
    "\n",
    "sns.barplot(data=importances,\n",
    "            order=order,  \n",
    "            x='importance',\n",
    "            y='feature',\n",
    "            errorbar=('ci', 95),\n",
    "            orient='h')\n",
    "\n",
    "plt.xlabel('Importance (average weight of tree-splits which use the feature)')\n",
    "plt.show()\n",
    "\n",
    "# plot feature-group importance\n",
    "sns.barplot(data=importances,\n",
    "            x='importance',\n",
    "            y='feature_group',\n",
    "            errorbar=('ci', 95),\n",
    "            orient='h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multipass & SHAP Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-explain\n",
    "\n",
    "\n",
    "# Loads three ML models (random forest, gradient-boosted tree, and logistic regression)\n",
    "# trained on a subset of the road surface temperature data from Handler et al. (2020).\n",
    "\n",
    "estimator = tuned_model.fit(X, y)\n",
    "X_features = estimator.get_feature_extractor().transform(X)\n",
    "feature_names = estimator.get_feature_extractor().get_feature_names_out()\n",
    "\n",
    "explainer = skexplain.ExplainToolkit(('xgb', estimator.get_classification_head()),\n",
    "                                     estimator_output='raw',\n",
    "                                     X=X_features,y=y, feature_names=feature_names)\n",
    "\n",
    "\n",
    "# SHAP\n",
    "shap_results = explainer.local_attributions(method='shap')['shap_values__xgb'].values\n",
    "shap_data = to_skexplain_importance(shap_results, estimator_name='xgb', \n",
    "                                      feature_names=feature_names, method='shap_sum')\n",
    "explainer.plot_importance(data=shap_data,\n",
    "                          panels = [('shap_sum', 'xgb')],\n",
    "                          plot_correlated_features=True,\n",
    "                          rho_threshold=0.7,\n",
    "                          figsize=(4, 2.5),\n",
    "                          method='shap')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# multipass\n",
    "perm_results = explainer.permutation_importance(n_vars=10, direction='forward', evaluation_fn='auc')\n",
    "explainer.plot_importance(data=perm_results,\n",
    "                          panels=[('forward_multipass', 'xgb')],\n",
    "                          plot_correlated_features=True,\n",
    "                          rho_threshold=0.7,\n",
    "                          figsize=(4, 2.5),\n",
    "                          )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop-Column Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DropCol feature importance\n",
    "\n",
    "def get_dropcol_importance(estimator, X_train, y_train, X_test, y_test, scoring='accuracy'):\n",
    "\n",
    "    estimator.fit(X, y)  # just fit if not already fitted\n",
    "    feature_names = estimator.get_feature_extractor().get_feature_names_out()\n",
    "    X_train_features = estimator.get_feature_extractor().transform(X_train)\n",
    "    X_test_features = estimator.get_feature_extractor().transform(X_test)\n",
    "    clf = estimator.get_classification_head()\n",
    "\n",
    "    importances = []\n",
    "    for i, feat_name in enumerate(feature_names):\n",
    "        X_train_drop = np.delete(X_train_features, i, axis=1)\n",
    "        X_test_drop = np.delete(X_test_features, i, axis=1)\n",
    "        clf.fit(X_train_drop, y_train)\n",
    "        importance = permutation_importance(clf,\n",
    "                                            X_test_drop,\n",
    "                                            y_test,\n",
    "                                            n_jobs=-1,\n",
    "                                            scoring=scoring)['importances_mean']\n",
    "        new_feature_names = list(feature_names).copy()\n",
    "        new_feature_names.remove(feat_name)\n",
    "        importance = list(zip(new_feature_names, importance))\n",
    "        importances.extend(importance)\n",
    "\n",
    "    return importances\n",
    "\n",
    "# run permutation importance in parallel\n",
    "importances = Parallel(n_jobs=-1, verbose=2)(\n",
    "    delayed(get_dropcol_importance)(\n",
    "        estimator = tuned_model,\n",
    "        X_train = X[train],\n",
    "        y_train = y[train],\n",
    "        X_test = X[test],\n",
    "        y_test = y[test])\n",
    "    for train, test in StratifiedShuffleSplit(n_splits=N_RUNS, test_size=TEST_SIZE).split(X, y)\n",
    ")\n",
    "pd.DataFrame([item for sublist in importances for item in sublist], columns=['feature', 'importance']).groupby('feature').mean().sort_values('importance', ascending=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
