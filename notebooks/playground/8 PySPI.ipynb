{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebooks extracts 216 connectivity measures using the `pyspi` package, and then train a AVGPvsNVGP classifier using the connectivity features.\n",
    "\n",
    "**:warning: Warning:** This notebook requires packages that are not compatible with the current version of Python. To run this notebook, you need to create a separate conda environment using the following command:\n",
    "\n",
    "```bash\n",
    "mamba create -n pyspi python=3.9 octave openjdk\n",
    "mamba activate pyspi\n",
    "pip install -U pyspi scikit-learn xgboost xarray h5netcdf\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import xarray as xr\n",
    "from pyspi.calculator import CalculatorFrame\n",
    "from pyspi.data import Data\n",
    "\n",
    "from src.multimodal.preprocessing import TimeseriesAggregator\n",
    "\n",
    "# we only aggregate the region-wise timeseries into network-wise timeseries\n",
    "preproc_pipe = TimeseriesAggregator(strategy='network')\n",
    "\n",
    "atlas = 'dosenbach2010'\n",
    "\n",
    "with xr.open_dataset(f'data/Julia2018/timeseries_{atlas}.nc5') as ds:\n",
    "    ds.load()\n",
    "    ds = preproc_pipe.fit_transform(ds)\n",
    "\n",
    "datasets = []\n",
    "process_type = ds['timeseries'].dims[-1]\n",
    "process_names = ds.coords[process_type].values\n",
    "for subject in ds.coords['subject'].values:\n",
    "    ts = ds.sel(subject=subject)['timeseries'].values.T\n",
    "    dataset = Data(ts, procnames=process_names, name=subject)\n",
    "    datasets.append(dataset)\n",
    "\n",
    "calc = CalculatorFrame(datasets=datasets, subset='fabfour',\n",
    "                       name=f'Julia2018_{atlas}_{process_type}',\n",
    "                       names=[d.name for d in datasets])\n",
    "calc.compute()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 14.63it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# spis = spi_calc.table\n",
    "# spis = s.columns.get_level_values(0).unique()\n",
    "\n",
    "# spi_calc._get_correlation_df()\n",
    "\n",
    "tables = {\n",
    "    c.name: c.table\n",
    "    for i, c in calc.calculators.itertuples()\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "spis = []\n",
    "\n",
    "for i, c in tqdm(calc.calculators.itertuples(), total=calc.n_calculators):\n",
    "    feats = c.table\n",
    "    feats.index.name = 'process_1'\n",
    "    feats = feats.reset_index()\n",
    "    feats.columns.names = ['spi', 'process_2']\n",
    "    melted = pd.melt(feats, id_vars='process_1', var_name=['spi', 'process_2'], value_name='value')\n",
    "    melted['process'] = melted.apply(lambda x: set(x[['process_1', 'process_2']]), axis=1)\n",
    "    melted = melted.groupby('spi').apply(lambda x: x.drop_duplicates('process'))\n",
    "    # melted.dropna(subset=['value'], inplace=True)\n",
    "    melted['process'] = melted['process_1'] + '-' + melted['process_2']\n",
    "    melted.drop(columns=['spi', 'process_1', 'process_2'], inplace=True)\n",
    "    melted.reset_index(level=0, inplace=True)\n",
    "    melted.reset_index(drop=True, inplace=True)\n",
    "    melted = melted.assign(subject=c.name, label=c.name[:4])\n",
    "    spis.append(melted)\n",
    "spi_df = pd.concat(spis)\n",
    "spi_df_wide = spi_df.pivot_table(index=['subject', 'label', 'spi'], columns=['process'], values='value', aggfunc='mean').reset_index()\n",
    "spi_df_wide.to_csv(f'data/Julia2018/spis_{atlas}_{process_type}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPI Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spi\n",
       "spearmanr-sq               0.639000\n",
       "cov_EmpiricalCovariance    0.631250\n",
       "di_gaussian                0.552125\n",
       "pec                        0.543125\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit, LeaveOneOut\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def score_spi(s):\n",
    "    print(s.name, '...', end=' ')\n",
    "    estimator = Pipeline([\n",
    "        # ('scaler', StandardScaler()),\n",
    "        # ('scale', MinMaxScaler(feature_range=(-1, 1))),\n",
    "        ('clf', SVC(kernel='linear'))\n",
    "        # ('clf', XGBClassifier())\n",
    "    ])\n",
    "    X = s.drop(columns=['subject', 'spi', 'label']).values\n",
    "    y = LabelEncoder().fit_transform(s['label'].values)\n",
    "    CV = StratifiedShuffleSplit(n_splits=1000, test_size=8)\n",
    "    # CV = LeaveOneOut()\n",
    "    score = cross_val_score(estimator, X, y, cv=CV, n_jobs=-1, scoring='accuracy')\n",
    "    print(f'acc={score.mean()}')\n",
    "    return score.mean()\n",
    "\n",
    "atlas = 'dosenbach2010'\n",
    "spi_df_wide = pd.read_csv(f'data/Julia2018/spis_{atlas}_{process_type}.csv')\n",
    "s = spi_df_wide.groupby(['spi']).apply(lambda x: x.isna().sum().sum())\n",
    "\n",
    "null_spis = s[s>0].index\n",
    "spi_df_wide = spi_df_wide.query('spi not in @null_spis')\n",
    "scores = spi_df_wide.groupby(['spi']).apply(score_spi).sort_values(ascending=False)\n",
    "clear_output()\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
