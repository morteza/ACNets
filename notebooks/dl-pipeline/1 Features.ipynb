{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features\n",
    "\n",
    "We are interested in the following features:\n",
    "\n",
    "- H1: region-averaged time-series\n",
    "- H2: region-level connectivities (from H1, optional: triu-k1)\n",
    "- H3: network-averaged time-series (from H1)\n",
    "- H4: network connectivity (from H3, optional: triu-k1)\n",
    "- H5: networks connectivity (from H2, optional: triu-k0)\n",
    "\n",
    "> Note that we are not going to take the upper triangular part of the connectivity matrix and full matrices are used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing_extensions import deprecated\n",
    "\n",
    "from src.acnets.pipeline import Parcellation, ConnectivityAggregator, \\\n",
    "                                ConnectivityExtractor, TimeseriesAggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@deprecated('Use full connectivity matrix instead')\n",
    "def flatten(ds, mask_k=0):\n",
    "\n",
    "    ds = ds.copy()\n",
    "    import numpy as np\n",
    "    node_type = ds['connectivity'].dims[-1][:-4]\n",
    "    c = ds['connectivity'].values\n",
    "    c[..., *np.tril_indices(c.shape[1], k=mask_k)] = np.nan\n",
    "    ds['connectivity'].values = c\n",
    "    c = ds['connectivity'].to_dataframe().dropna().reset_index().rename(columns={\n",
    "        'connectivity': 'partial correlation'})\n",
    "    c['connectivity'] = c[f'{node_type}_src'] + ' - ' + c[f'{node_type}_dst']\n",
    "    c = c.pivot(index='subject', columns='connectivity', values='partial correlation')\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlases = [\n",
    "    'dosenbach2010',\n",
    "    'dosenbach2007',\n",
    "    'difumo_64_2mm',\n",
    "    'difumo_128_2mm',\n",
    "    'difumo_256_2mm',\n",
    "    'difumo_512_2mm',\n",
    "    'difumo_1024_2mm',\n",
    "    'gordon2014_2mm',\n",
    "    'cort-maxprob-thr25-2mm',\n",
    "    'seitzman2018',\n",
    "    'friedman2020',\n",
    "]\n",
    "\n",
    "kinds = [\n",
    "    'correlation',\n",
    "    'partial correlation',\n",
    "    'tangent',\n",
    "    'covariance',\n",
    "    'precisions'\n",
    "]\n",
    "\n",
    "h1_time_regions = Parcellation(atlas_name='dosenbach2010', verbose=1).fit_transform(X=None)\n",
    "h2_conn_regions = ConnectivityExtractor(kind='partial correlation').fit_transform(h1_time_regions)\n",
    "h3_time_networks = TimeseriesAggregator(strategy='network').fit_transform(h1_time_regions)\n",
    "h4_conn_networks = ConnectivityExtractor(kind='partial correlation').fit_transform(h3_time_networks)\n",
    "h5_conn_networks = ConnectivityAggregator(strategy='network').fit_transform(h2_conn_regions)\n",
    "\n",
    "# TODO WIP: reshape as tidy dataframes\n",
    "\n",
    "# h1\n",
    "# h1 = h1_time_regions['timeseries'].to_dataframe().reset_index().pivot(index=['subject', 'region'], columns='timepoint')\n",
    "# h1.columns = h1.columns.droplevel(0)\n",
    "h1 = h1_time_regions['timeseries'].values\n",
    "\n",
    "# h2\n",
    "# h2 = flatten(h2_conn_regions, mask_k=0)\n",
    "h2 = h2_conn_regions['connectivity'].values\n",
    "\n",
    "# h3\n",
    "# h3 = h3_time_networks['timeseries'].to_dataframe().reset_index().pivot(index=['subject', 'network'], columns='timepoint')\n",
    "# h3.columns = h3.columns.droplevel(0)\n",
    "h3 = h3_time_networks['timeseries'].values\n",
    "\n",
    "# h4\n",
    "# h4 = flatten(h4_conn_networks, mask_k=0)\n",
    "h4 = h4_conn_networks['connectivity'].values\n",
    "\n",
    "# h5\n",
    "# h5 = flatten(h5_conn_networks, mask_k=-1)\n",
    "h5 = h5_conn_networks['connectivity'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 124, 160])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset, ConcatDataset\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "        def __init__(self, atlas='dosenbach2010', kind='partial correlation',\n",
    "                     test_ratio=.25, val_ratio=.125):\n",
    "            super().__init__()\n",
    "            self.atlas = atlas\n",
    "            self.kind = kind\n",
    "            self.test_ratio = test_ratio\n",
    "            self.val_ratio = val_ratio\n",
    "            self.train_ratio = 1 - test_ratio - val_ratio\n",
    "    \n",
    "        def prepare_data(self):\n",
    "            h1_time_regions = Parcellation(atlas_name=self.atlas).fit_transform(X=None)\n",
    "            h2_conn_regions = ConnectivityExtractor(kind=self.kind).fit_transform(h1_time_regions)\n",
    "            h3_time_networks = TimeseriesAggregator(strategy='network').fit_transform(h1_time_regions)\n",
    "            h4_conn_networks = ConnectivityExtractor(kind=self.kind).fit_transform(h3_time_networks)\n",
    "            h5_conn_networks = ConnectivityAggregator(strategy='network').fit_transform(h2_conn_regions)\n",
    "            h1 = torch.Tensor(h1_time_regions['timeseries'].values)\n",
    "            h2 = torch.Tensor(h2_conn_regions['connectivity'].values)\n",
    "            h3 = torch.Tensor(h3_time_networks['timeseries'].values)\n",
    "            h4 = torch.Tensor(h4_conn_networks['connectivity'].values)\n",
    "            h5 = torch.Tensor(h5_conn_networks['connectivity'].values)\n",
    "\n",
    "            data = TensorDataset(h1, h2, h3, h4, h5)\n",
    "            n_subjects = len(data)\n",
    "            self.train, self.val, self.test = random_split(\n",
    "                data,\n",
    "                [int(n_subjects * self.train_ratio),\n",
    "                 int(n_subjects * self.val_ratio),\n",
    "                 int(n_subjects * self.test_ratio)])\n",
    "\n",
    "            # TODO concat val to train\n",
    "            # self.train = ConcatDataset([self.train, self.val])\n",
    "\n",
    "        def setup(self, stage=None):\n",
    "            pass\n",
    "    \n",
    "        def train_dataloader(self):\n",
    "            return DataLoader(self.train, batch_size=8)\n",
    "    \n",
    "        def val_dataloader(self):\n",
    "            return DataLoader(self.val, batch_size=8)\n",
    "    \n",
    "        def test_dataloader(self):\n",
    "            return DataLoader(self.test, batch_size=8)\n",
    "\n",
    "datamodule = DataModule(atlas='dosenbach2010', kind='partial correlation')\n",
    "datamodule.prepare_data()\n",
    "\n",
    "datamodule.train_dataloader().dataset[:][0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
