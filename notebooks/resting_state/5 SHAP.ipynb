{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import feature_selection, svm, preprocessing, model_selection, ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import xgboost\n",
    "import shap\n",
    "\n",
    "from nilearn import datasets as nilean_datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set('paper')\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "# Technical reproducibility\n",
    "%reload_ext watermark\n",
    "%watermark -iv -co -ituhmv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "TRIU_K = 0\n",
    "DATASET_NAME = 'dosenbach2010_tangent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlases = ['dosenbach2007', 'dosenbach2010', 'difumo_64_2',]# 'difumo_128_2',]# 'difumo_1024_2']\n",
    "\n",
    "connectivity_measures = ['tangent', 'precision', 'correlation',\n",
    "                         'covariance', 'partial_correlation']\n",
    "\n",
    "DATASETS = dict()\n",
    "\n",
    "for atlas in atlases:\n",
    "  for connectivity in connectivity_measures:\n",
    "    _conn_key = f'{connectivity}_connectivity'\n",
    "    ds = xr.open_dataset(f'data/julia2018_resting/connectivity_{atlas}.nc')\n",
    "    _conn = ds[_conn_key]\n",
    "    _conn.coords['group'] = ds.group\n",
    "    _conn['inverse_efficiency_score_ms'] = ds['inverse_efficiency_score_ms']\n",
    "    DATASETS[f'{atlas}_{connectivity}'] = _conn\n",
    "    \n",
    "    if 'difumo_names' in ds.coords:\n",
    "      _conn.coords['region'] = ds.coords['difumo_names'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DATASETS[DATASET_NAME]\n",
    "\n",
    "behavioral_scores = dataset['inverse_efficiency_score_ms'].values\n",
    "\n",
    "# remove subjects with missing behavioral data or duplicate scanning sessions\n",
    "# subject_labels = xr.concat([dataset['subject'], dataset['subject'] + 'NEW'], dim='subject')\n",
    "# invalid_subjects = subject_labels.to_series().duplicated(keep='first')[32:]\n",
    "# invalid_subjects = invalid_subjects | np.isnan(behavioral_scores)\n",
    "invalid_subjects = np.isnan(behavioral_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = dataset.coords['region'].values\n",
    "\n",
    "feature_names = pd.DataFrame(\n",
    "  np.empty((len(regions), len(regions))),\n",
    "  index=regions, columns=regions)\n",
    "\n",
    "feature_names = feature_names.apply(lambda x: x.index + ' \\N{left right arrow} ' + x.name)\n",
    "feature_names = feature_names.values[np.triu_indices_from(feature_names.values, k=TRIU_K)]\n",
    "\n",
    "# X\n",
    "X = np.array([subj_conn[np.triu_indices_from(subj_conn, k=TRIU_K)]\n",
    "              for subj_conn in dataset.values])\n",
    "\n",
    "X_threshold = np.median(X, axis=1) + np.std(X, axis=1)\n",
    "X = np.where(np.abs(X) >= X_threshold.reshape(-1,1), X, 0)\n",
    "\n",
    "# y\n",
    "y_encoder = preprocessing.LabelEncoder()\n",
    "y = y_encoder.fit_transform(dataset['group'])\n",
    "\n",
    "# remove subjects with missing behavioral data\n",
    "X = X[~invalid_subjects]\n",
    "y = y[~invalid_subjects]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ALT 1: SVM\n",
    "model = Pipeline([\n",
    "    ('zerovar', feature_selection.VarianceThreshold(.01)),\n",
    "    ('model', svm.SVC(kernel='rbf', C=1, probability=True)),\n",
    "], verbose=False)\n",
    "\n",
    "# ALT2: RandomForest\n",
    "model = ensemble.RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1)\n",
    "\n",
    "# ALT3: XGBoost\n",
    "model = xgboost.XGBClassifier(\n",
    "    n_estimators=100, max_depth=10,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    n_jobs=-1)\n",
    "\n",
    "# TODO hyper-parameter tuning\n",
    "\n",
    "train, test = model_selection.train_test_split(\n",
    "  range(len(X)),\n",
    "  test_size=0.5,\n",
    "  shuffle=True,\n",
    "  stratify=y,\n",
    ")\n",
    "\n",
    "model.fit(X[train], y[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping...\n",
    "\n",
    "# DEBUG /start\n",
    "\n",
    "test_score = model_selection.cross_val_score(\n",
    "    model, X, y, n_jobs=-1, scoring='roc_auc',\n",
    "    cv = model_selection.RepeatedStratifiedKFold(n_splits=5, n_repeats=20),\n",
    ")\n",
    "\n",
    "print(f'mean(CV-AUC): {test_score.mean():.2f}')\n",
    "\n",
    "perm_score, _, pvalue = model_selection.permutation_test_score(\n",
    "    model, X, y,\n",
    "    cv = model_selection.RepeatedStratifiedKFold(n_splits=5, n_repeats=10),\n",
    "    n_jobs=-1,\n",
    "    n_permutations=10,\n",
    "    # cv=5,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "print(f'Permutation test AUC: {perm_score:.2f} (p-value={pvalue:.3f})')\n",
    "\n",
    "# DEBUG /end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV-SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_cv = []\n",
    "X_test_indices_cv = []\n",
    "y_test_cv = []\n",
    "y_pred_cv = []\n",
    "expected_value_cv = []\n",
    "\n",
    "cv = model_selection.RepeatedStratifiedKFold(n_splits=5, n_repeats=200)\n",
    "\n",
    "n_splits = cv.get_n_splits(X, y)\n",
    "\n",
    "for train, test in tqdm(cv.split(X, y), total=n_splits):\n",
    "\n",
    "    # train the model\n",
    "    model.fit(X[train], y[train])\n",
    "    y_pred = model.predict(X[test])\n",
    "    \n",
    "    # # fit explainer\n",
    "    explainer = shap.Explainer(\n",
    "        model, X[train],\n",
    "        feature_names=feature_names,\n",
    "        algorithm='tree',\n",
    "        # output_names=dataset['group'].values[train],\n",
    "        # feature_perturbation='tree_path_dependent'\n",
    "    )\n",
    "\n",
    "    # evaluate explainer\n",
    "    # Note: for Permutation explainer, add max_evals= 100 * X.shape[1] + 1\n",
    "    shap_values = explainer(X[test])\n",
    "    # shap_interaction_values = explainer.shap_interaction_values(X[test])\n",
    "\n",
    "    shap_values_cv.append(shap_values)\n",
    "    expected_value_cv.append(explainer.expected_value)\n",
    "    X_test_indices_cv.append(test)\n",
    "    y_test_cv.append(y[test])\n",
    "    y_pred_cv.append(y_pred)\n",
    "\n",
    "# merge CV results\n",
    "shap_values = np.vstack([sh_val.values for sh_val in shap_values_cv])\n",
    "X_test = pd.DataFrame(X[np.hstack(X_test_indices_cv)], columns=feature_names)\n",
    "y_test = np.hstack(y_test_cv)\n",
    "y_pred = np.hstack(y_pred_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = shap.Explanation(\n",
    "  values = np.vstack([sh.values for sh in shap_values_cv]),\n",
    "  base_values = np.hstack([sh.base_values for sh in shap_values_cv]),\n",
    "  data = np.vstack([sh.data for sh in shap_values_cv]),\n",
    "  feature_names=shap_values_cv[0].feature_names,\n",
    "  compute_time=np.sum([sh.compute_time for sh in shap_values_cv]),\n",
    "  output_names=y_encoder.classes_,\n",
    "  output_indexes=y_pred,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, max_display=20, alpha=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use explainers to calculate base and subsample to speed up plotting\n",
    "# shap.force_plot(np.mean(expected_values_cv), shap_values, X_test, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = None\n",
    "# clustering = shap.utils.hclust(X_test.iloc[:,:10], y_test)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = y_pred != y_test\n",
    "\n",
    "shap.plots.decision(np.mean(expected_value_cv),\n",
    "                    shap_values.values,#[misclassified],\n",
    "                    feature_names=feature_names.tolist(),\n",
    "                    # feature_display_range=range(10, -1, -1),\n",
    "                    link='logit',\n",
    "                    # feature_order='hclust',\n",
    "                    highlight=misclassified,\n",
    "                    legend_labels=y_encoder.classes_.tolist()\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_name(region, dataset_name=None):\n",
    "\n",
    "  if 'difumo' in dataset_name.lower():\n",
    "    atlas = nilean_datasets.fetch_atlas_difumo(\n",
    "      dimension=64, resolution_mm=2, legacy_format=False)\n",
    "\n",
    "    labels = atlas.labels.set_index('difumo_names')\n",
    "    \n",
    "    return labels.loc[region,'yeo_networks17']\n",
    "  \n",
    "  elif 'dosenbach2010' in dataset_name.lower():\n",
    "    atlas = nilean_datasets.fetch_coords_dosenbach_2010(legacy_format=False)\n",
    "    labels = pd.concat(\n",
    "      [pd.DataFrame(v) for k, v in atlas.items() if k != 'description'], axis=1)\n",
    "    labels.set_index(0, inplace=True)\n",
    "    \n",
    "    return labels.loc[region,'network']\n",
    "\n",
    "  raise Exception('Invalid atlas name.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap2d_size = len(dataset.coords['region'])\n",
    "\n",
    "agg_shap_values = np.abs(shap_values.values).sum(axis=0)\n",
    "\n",
    "\n",
    "shap2d_values = np.zeros((shap2d_size, shap2d_size))\n",
    "shap2d_triu_indices = np.triu_indices(shap2d_size, k=TRIU_K)\n",
    "shap2d_values[shap2d_triu_indices] = agg_shap_values\n",
    "shap2d_values = shap2d_values + shap2d_values.T - np.diag(np.diag(shap2d_values))\n",
    "\n",
    "shap2d = pd.DataFrame(shap2d_values,\n",
    "                      index=dataset.coords['region'],\n",
    "                      columns=dataset.coords['region'])\n",
    "\n",
    "\n",
    "sorted_shap_indices = np.argsort(agg_shap_values)[::-1]\n",
    "top_n = 10\n",
    "triu_idx = sorted_shap_indices[:top_n]\n",
    "row_idx = np.triu_indices(shap2d_size, k=TRIU_K)[0][triu_idx]\n",
    "col_idx = np.triu_indices(shap2d_size, k=TRIU_K)[1][triu_idx]\n",
    "\n",
    "# DEBUG make sure indices are mapped correctly\n",
    "assert np.all(shap2d_values[row_idx,col_idx] == agg_shap_values[triu_idx])\n",
    "\n",
    "\n",
    "print('Top contributing connectivities:')\n",
    "for i, (row, col) in enumerate(zip(row_idx, col_idx)):\n",
    "  # print(f'{shap2d.index[i]} - {shap2d.columns[j]}')\n",
    "  row_region = dataset.coords['region'].values[row]\n",
    "  col_region = dataset.coords['region'].values[col]\n",
    "  row_net = get_network_name(row_region, DATASET_NAME)\n",
    "  col_net = get_network_name(col_region, DATASET_NAME)  \n",
    "  print(f'{i+1}) {row_region} \\N{left right arrow} {col_region} '\n",
    "        f'[{row_net} \\N{left right arrow} {col_net}]')\n",
    "  \n",
    "# Top contributing connectivities:\n",
    "# 1) sup parietal 86 ↔ sup parietal 86 [occipital ↔ occipital]\n",
    "# 2) vFC 40 ↔ vFC 40 [cingulo-opercular ↔ cingulo-opercular]\n",
    "# 3) vlPFC 12 ↔ vlPFC 12 [default ↔ default]\n",
    "# 4) IPS 134 ↔ IPS 134 [sensorimotor ↔ sensorimotor]\n",
    "# 5) occipital 92 ↔ occipital 92 [sensorimotor ↔ sensorimotor]\n",
    "# 6) post occipital 153 ↔ post occipital 153 [default ↔ default]\n",
    "# 7) mid insula 56 ↔ mid insula 56 [occipital ↔ occipital]\n",
    "# 8) occipital 137 ↔ occipital 137 [sensorimotor ↔ sensorimotor]\n",
    "# 9) angular gyrus 124 ↔ angular gyrus 124 [sensorimotor ↔ sensorimotor]\n",
    "# 10) mPFC 4 ↔ mPFC 4 [sensorimotor ↔ sensorimotor]\n",
    "\n",
    "# Top contributing connectivities:\n",
    "# 1) sup parietal 86 ↔ sup parietal 86 [occipital ↔ occipital]\n",
    "# 2) vFC 40 ↔ vFC 40 [cingulo-opercular ↔ cingulo-opercular]\n",
    "# 3) IPS 134 ↔ IPS 134 [sensorimotor ↔ sensorimotor]\n",
    "# 4) vlPFC 12 ↔ vlPFC 12 [default ↔ default]\n",
    "# 5) occipital 137 ↔ occipital 137 [sensorimotor ↔ sensorimotor]\n",
    "# 6) post occipital 153 ↔ post occipital 153 [default ↔ default]\n",
    "# 7) mid insula 56 ↔ mid insula 56 [occipital ↔ occipital]\n",
    "# 8) occipital 92 ↔ occipital 92 [sensorimotor ↔ sensorimotor]\n",
    "# 9) angular gyrus 124 ↔ angular gyrus 124 [sensorimotor ↔ sensorimotor]\n",
    "# 10) mPFC 4 ↔ mPFC 4 [sensorimotor ↔ sensorimotor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_names = shap2d.index.to_frame(name='network').apply(\n",
    "  lambda x: get_network_name(x, DATASET_NAME)\n",
    "  )\n",
    "\n",
    "palt = dict(zip(network_names['network'].unique(),\n",
    "                sns.color_palette('Set1', network_names.nunique()['network'])))\n",
    "\n",
    "row_colors = network_names.apply(\n",
    "  lambda x: pd.Series((palt[x['network']], x['network'])), axis=1)\n",
    "row_colors.rename(columns={0:'color', 1:'network'}, inplace=True)\n",
    "row_colors.index.name = 'region'\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "row_colors_legend = [mpatches.Patch(color=c, label=n)\n",
    "                     for l,c,n in row_colors.drop_duplicates('network').itertuples()]\n",
    "\n",
    "g = sns.clustermap(\n",
    "  shap2d,\n",
    "  figsize=(10,10),\n",
    "  row_colors=row_colors[['color']],\n",
    "  robust=True,\n",
    "  dendrogram_ratio=0.0001,\n",
    "  cbar_pos=(1.1, .79, 0.01, 0.2),\n",
    "  cmap='Blues')\n",
    "\n",
    "legend2=g.ax_heatmap.legend(\n",
    "  loc='center left',\n",
    "  bbox_to_anchor=(1.25,0.65),\n",
    "  handles=row_colors_legend,\n",
    "  frameon=True)\n",
    "\n",
    "plt.suptitle('Clustered SHAP values\\n'\n",
    "             'Notes: Color bar shows the brain networks. '\n",
    "             'Only a subset of labels are shown.', x=0.02, y=1.02, ha='left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "## interactive heatmap (but not clustered)\n",
    "# fig = px.imshow(shap2d, aspect='auto', height=800)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nilearn import plotting\n",
    "\n",
    "if 'dosenbach2010' in DATASET_NAME:\n",
    "  atlas = nilean_datasets.fetch_coords_dosenbach_2010(legacy_format=False)\n",
    "  atlas_coordinates = atlas['rois'].values\n",
    "elif 'difumo' in DATASET_NAME:\n",
    "  atlas = nilean_datasets.fetch_atlas_difumo(64, 2, legacy_format=False)\n",
    "  atlas_coordinates = plotting.find_probabilistic_atlas_cut_coords(maps_img=atlas.maps)\n",
    "  labels = atlas.labels.set_index('difumo_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# METHOD 1: aggregate all shaps for each node\n",
    "# node_strength = np.sum(shap2d.values, axis=1).reshape(-1, 1)\n",
    "# node_strength = preprocessing.StandardScaler().fit_transform(shap2d.values)\n",
    "# node_strength = node_strength.sum(axis=1) * 4\n",
    "\n",
    "# METHOD 2: just use the node's self edge strength\n",
    "node_strength = np.diag(shap2d) / 2\n",
    "\n",
    "plotting.plot_connectome(\n",
    "  shap2d, atlas_coordinates,\n",
    "  node_color=row_colors['color'],\n",
    "  colorbar=True,\n",
    "  node_size=node_strength,\n",
    "  title='SHAP values of the edges and aggregated node values.',\n",
    "  edge_threshold='95%',)\n",
    "\n",
    "plt.gca().legend(\n",
    "  loc='center left',\n",
    "  bbox_to_anchor=(2,0.5),\n",
    "  handles=row_colors_legend,\n",
    "  frameon=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_strength = np.diag(shap2d)\n",
    "\n",
    "plotting.plot_markers(\n",
    "    node_strength,\n",
    "    atlas_coordinates,\n",
    "    node_size=node_strength / 4,\n",
    "    title='Node strength (SHAP values)',\n",
    "    node_cmap='Blues'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = plotting.view_connectome(\n",
    "    shap2d.values, atlas_coordinates,\n",
    "    node_color=row_colors['color'],\n",
    "    node_size=np.diag(shap2d)/20, edge_threshold=10,\n",
    "    colorbar_fontsize=12,\n",
    "    title=f'SHAP values ({DATASET_NAME})')\n",
    "\n",
    "view"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34f04479ffaeb5c00adb9e28a92647dce776275bf5ee61de72266754f4451f1a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('acnets')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
