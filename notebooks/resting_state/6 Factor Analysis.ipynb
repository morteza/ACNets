{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACNets: Confirmatory Factor Analysis\n",
    "\n",
    "We use dosenbach2007 and dosenbach2010 networks in a confirmatory factor analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATLAS = 'dosenbach2010'\n",
    "CONNECTIVITY_MEASURE = 'tangent'\n",
    "FIT_ONLY_INTER_NETWORK_CONNECTIVITIES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2022-02-24T11:34:26.097033+01:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.9.10\n",
      "IPython version      : 8.0.1\n",
      "\n",
      "conda environment: acnets\n",
      "\n",
      "Compiler    : Clang 11.1.0 \n",
      "OS          : Darwin\n",
      "Release     : 21.3.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 12\n",
      "Architecture: 64bit\n",
      "\n",
      "Hostname: MP0159\n",
      "\n",
      "seaborn   : 0.11.2\n",
      "pandas    : 1.4.0\n",
      "re        : 2.2.1\n",
      "numpy     : 1.21.5\n",
      "matplotlib: 3.5.1\n",
      "nilearn   : 0.9.0\n",
      "sys       : 3.9.10 | packaged by conda-forge | (main, Feb  1 2022, 21:28:27) \n",
      "[Clang 11.1.0 ]\n",
      "xarray    : 0.21.1\n",
      "sklearn   : 1.0.2\n",
      "skopt     : 0.9.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set('notebook')\n",
    "\n",
    "from sklearn import preprocessing, model_selection, metrics, ensemble, multioutput\n",
    "from sklearn import decomposition, cross_decomposition, feature_selection, dummy, svm\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from nilearn import datasets as nilean_datasets\n",
    "\n",
    "import skopt\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "from python.acnets.datasets import load_connectivity\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from factor_analyzer import ConfirmatoryFactorAnalyzer, ModelSpecificationParser\n",
    "\n",
    "# Technical reproducibility\n",
    "%reload_ext watermark\n",
    "%watermark -iv -co -ituhmv\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = model_selection.StratifiedKFold(5)\n",
    "\n",
    "pipe = Pipeline([\n",
    "  ('zv', feature_selection.VarianceThreshold()),\n",
    "  ('clf', svm.SVC(kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "param_space = {\n",
    "  'clf__C': Real(1e-3, 1e3, 'log-uniform'),\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(pipe, param_space, cv=cv, n_jobs=1, scoring='roc_auc')\n",
    "\n",
    "\n",
    "def binarize(X):\n",
    "  \"\"\"binarize thr connectivity matrix by median ± std threshold.\"\"\"\n",
    "\n",
    "  X_threshold = np.array([np.median(x, keepdims=True) + x.std(keepdims=True) for x in X])\n",
    "  X = np.where(np.abs(X) > X_threshold, 1, 0)\n",
    "\n",
    "  return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "X is a 2D matrix of size (n_subjects, n_features), where features are the connectivity measures, e.g., correlation.\n",
    "\n",
    "We first load the data and preprocess it, i.e., binarize the X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, feature_names = load_connectivity(\n",
    "  parcellation=ATLAS,\n",
    "  kind=CONNECTIVITY_MEASURE,\n",
    "  vectorize=False,\n",
    "  return_y=True,\n",
    "  only_diagonal=False,\n",
    "  return_feature_names=True,\n",
    "  discard_diagonal=False,\n",
    "  discard_cerebellum=False,)\n",
    "\n",
    "# binarize X by median ± std threshold\n",
    "# X = binarize(X)\n",
    "\n",
    "# encode ys as integers\n",
    "y_encoder = preprocessing.LabelEncoder()\n",
    "y = y_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, extract network names from the atlas. `network_names` is data frame in that `reigon`s are mapped to `network`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ATLAS.lower() == 'dosenbach2010':\n",
    "  coords = nilean_datasets.fetch_coords_dosenbach_2010(legacy_format=False)\n",
    "  network_names = pd.concat(\n",
    "    [coords['rois'].reset_index(drop=True),\n",
    "     pd.Series(coords['labels']),\n",
    "     coords['networks'].reset_index(drop=True)], axis=1)\n",
    "  network_names.set_index(0, inplace=True)\n",
    "  network_names.index.name = 'region'\n",
    "\n",
    "elif ATLAS.lower() == 'dosenbach2007':\n",
    "  network_names = pd.read_csv('data/dosenbach2007/ROIS.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirmatory factor analysis\n",
    "Now, we programmatically define the factor model. Factor model is a dictionary that maps latent factors, i.e., brain networks, to the observable variables, i.e., node features within the brain networks.\n",
    "\n",
    "We then fit the factor model to the data and extract the factor loadings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_factor_name(feature, network_names=network_names):\n",
    "  \"\"\"Get factor given observable variable name.\"\"\"\n",
    "\n",
    "  if not '↔' in feature:\n",
    "    return network_names.loc[feature, 'network']\n",
    "  \n",
    "  src, tgt = feature.split(' ↔ ')\n",
    "  src_net = network_names.loc[src, 'network']\n",
    "  tgt_net = network_names.loc[tgt, 'network']\n",
    "  if src_net == tgt_net:\n",
    "    return src_net\n",
    "  else:\n",
    "    return '{} ↔ {}'.format(src_net, tgt_net)\n",
    "\n",
    "\n",
    "net_feature_names = feature_names.applymap(get_factor_name, network_names=network_names)\n",
    "net_feature_names = net_feature_names.values[np.triu_indices_from(net_feature_names.values, k=1)]\n",
    "\n",
    "factors_model = dict.fromkeys(pd.Series(net_feature_names).unique(), [])\n",
    "\n",
    "unique_feature_names = feature_names.values[np.triu_indices_from(feature_names.values, k=1)]\n",
    "\n",
    "for feat_name in unique_feature_names:\n",
    "  net_name = get_factor_name(feat_name)\n",
    "  factors_model[net_name] = factors_model[net_name] + [feat_name]\n",
    "\n",
    "# flatten X and feature names into 2D and 1D arrays\n",
    "_X2d = np.array([x[np.triu_indices_from(x, k=1)] for x in X])\n",
    "_feature_names_1d = feature_names.values[np.triu_indices_from(feature_names.values, k=1)]\n",
    "\n",
    "# store flattened X along with the feature names in a dataframe\n",
    "X_df = pd.DataFrame(_X2d, columns=_feature_names_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factor model analyzer uses SciPy optimizer under the hood. It's slow for our data, so we limit the features to those within the networks and discard intra-network connections. This will produce a dataset of 118 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIT_ONLY_INTER_NETWORK_CONNECTIVITIES:\n",
    "  factors_model = {k:v for k,v in factors_model.items() if '↔' not in k}\n",
    "  feature_names = []\n",
    "  for v in factors_model.values():\n",
    "    feature_names.extend(v)\n",
    "  feature_names = list(set(feature_names))\n",
    "  X_df = X_df.loc[:, feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2 (p-value): -inf (1.0)\n",
      "kmo total score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/acnets/lib/python3.9/site-packages/factor_analyzer/factor_analyzer.py:111: RuntimeWarning: divide by zero encountered in log\n",
      "  statistic = -np.log(corr_det) * (n - 1 - (2 * p + 5) / 6)\n"
     ]
    }
   ],
   "source": [
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
    "\n",
    "# Adequacy Test\n",
    "\n",
    "chi2_value, chi2_pvalue = calculate_bartlett_sphericity(X_df)\n",
    "kmo_all, kmo_total_score = calculate_kmo(X_df)\n",
    "\n",
    "print(\n",
    "  'chi2 (p-value): {} ({})\\n'\n",
    "  'kmo total score: {}'.format(chi2_value, chi2_pvalue, kmo_total_score)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_spec = ModelSpecificationParser.parse_model_specification_from_dict(\n",
    "  X_df, factors_model)\n",
    "\n",
    "cfa = ConfirmatoryFactorAnalyzer(factors_spec, disp=False) \n",
    "\n",
    "# X_norm = preprocessing.StandardScaler().fit_transform(X_df)\n",
    "X_cfa = cfa.fit_transform(X_df, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the factor model\n",
    "\n",
    "Extracted latent features can not be used for prediction.\n",
    "\n",
    "The following cell perform a permutation test to contrast observed and random scores (ROC AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = model_selection.RepeatedStratifiedKFold(n_splits=5, n_repeats=10)\n",
    "\n",
    "# test/train splits\n",
    "train, test = model_selection.train_test_split(\n",
    "  range(len(X)),\n",
    "  test_size=0.2,\n",
    "  shuffle=True,\n",
    "  stratify=y,)\n",
    "\n",
    "\n",
    "# model fitting\n",
    "try:\n",
    "  progress_bar = tqdm(total=opt.total_iterations)\n",
    "  opt.fit(\n",
    "    X_cfa, y,\n",
    "    callback = [\n",
    "      skopt.callbacks.DeadlineStopper(total_time=120),\n",
    "      lambda _: False if progress_bar.update() else False,\n",
    "  ])\n",
    "finally:\n",
    "  progress_bar.clear()\n",
    "  progress_bar.close()\n",
    "\n",
    "\n",
    "# permutation testing\n",
    "obs_score, rnd_scores, obs_pvalue = model_selection.permutation_test_score(\n",
    "  opt.best_estimator_,\n",
    "  X_cfa, y,\n",
    "  cv=cv,\n",
    "  n_permutations=10,\n",
    "  scoring='roc_auc')\n",
    "\n",
    "print('Observed score (p-value): {:.3f} ({:.3f})'.format(obs_score, obs_pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = sns.displot(rnd_scores, kde=True)\n",
    "g.set(xlabel='ROC AUC')\n",
    "g.ax.set_title('{} {}'.format(ATLAS, CONNECTIVITY_MEASURE))\n",
    "\n",
    "plt.axvline(obs_score, ls='--', color='blue')\n",
    "\n",
    "\n",
    "plt.text(x=obs_score + .02,\n",
    "        y=plt.gca().get_ylim()[1] * .7,\n",
    "        s=f'AUC = {obs_score:.2f}\\n(p-value: {obs_pvalue:.3f})')\n",
    "\n",
    "plt.suptitle(f'{cv.n_repeats} repeats of 5-fold cross-validated permutation testing\\n'\n",
    "             f'Dashed line is the observed scores without permutation '\n",
    "             f'and blue curves represents $H_0$ (10 permuted y labels)',\n",
    "             y=1.2, x=.08, ha='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we visualize the factor loadings against the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palt = dict(zip(\n",
    "  network_names['network'].unique(),\n",
    "  sns.color_palette('Set1', network_names['network'].nunique())))\n",
    "\n",
    "network_colors = network_names['network'].apply(lambda x: pd.Series((palt[x], x)))\n",
    "network_colors.rename(columns={0:'color', 1:'network'}, inplace=True)\n",
    "network_colors.reset_index(drop=True, inplace=True)\n",
    "network_colors.drop_duplicates(inplace=True)\n",
    "network_colors.set_index('network', inplace=True)\n",
    "\n",
    "\n",
    "feature_colors = {}\n",
    "for k,v in factors_model.items():\n",
    "  for vv in v:\n",
    "    feature_colors[vv] = network_colors.loc[k, 'color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_loadings = pd.DataFrame(\n",
    "  cfa.loadings_,\n",
    "  index=feature_names,\n",
    "  columns=list(factors_model.keys())\n",
    ")\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(30,3))\n",
    "# sns.heatmap(fa_loadings.T, ax=ax)\n",
    "\n",
    "\n",
    "g = sns.clustermap(fa_loadings.T, figsize=(25,5),\n",
    "               row_cluster=False, col_cluster=True,\n",
    "               cbar_pos=(.975, .25, 0.005, 0.3),\n",
    "               cmap='RdBu',\n",
    "               xticklabels=1,\n",
    "               colors_ratio=.07,\n",
    "               dendrogram_ratio=.3,\n",
    "               col_colors=list(feature_colors.values()),)\n",
    "\n",
    "g.ax_row_dendrogram.set_visible(False)\n",
    "\n",
    "# to rotate y labels\n",
    "# g.ax_heatmap.set_xticklabels(g.ax_heatmap.get_xticklabels(), rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also report the importance of each factor, that is coefficient of the linear SVM classifier we trained above.\n",
    "\n",
    "\n",
    "It can be seen as a measure of the importance of each factor to the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (net, feats) in enumerate(factors_model.items()):\n",
    "  coef = opt.best_estimator_['clf'].coef_[0][i]\n",
    "  print(f'{net}:\\n'\n",
    "        f'\\tSVM coef: {coef:.2f}\\n'\n",
    "        f'\\tfeatures: {\", \".join(feats)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP\n",
    "\n",
    "We can explain the contribution of each factor to the classification score by plotting their SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "cv = model_selection.RepeatedStratifiedKFold(n_splits=5, n_repeats=100)\n",
    "n_splits = cv.get_n_splits(X, y)\n",
    "\n",
    "shap_values_cv = []\n",
    "expected_value_cv = []\n",
    "X_test_indices_cv = []\n",
    "y_test_cv = []\n",
    "y_pred_cv = []\n",
    "\n",
    "model = opt.best_estimator_\n",
    "\n",
    "for train, test in tqdm(cv.split(X, y), total=n_splits):\n",
    "\n",
    "  model.fit(X_cfa[train], y[train])\n",
    "  y_pred = model.predict(X_cfa[test])\n",
    "  explainer = shap.Explainer(model.predict_proba,\n",
    "                            X_cfa[train],\n",
    "                            feature_names=list(factors_model.keys()))\n",
    "\n",
    "  shap_values = explainer(X_cfa[test])\n",
    "\n",
    "  shap_values_cv.append(shap_values)\n",
    "  # expected_value_cv.append(explainer.expected_value)\n",
    "  X_test_indices_cv.append(test)\n",
    "  y_test_cv.append(y[test])\n",
    "  y_pred_cv.append(y_pred)\n",
    "\n",
    "# merge CV data\n",
    "y_test = np.hstack(y_test_cv)\n",
    "y_pred = np.hstack(y_pred_cv)\n",
    "\n",
    "# merge CV SHAPs\n",
    "shap_values = shap.Explanation(\n",
    "  values = np.vstack([sh.values[...,1] for sh in shap_values_cv]),\n",
    "  base_values = np.hstack([sh.base_values[...,1] for sh in shap_values_cv]),\n",
    "  data = np.vstack([sh.data for sh in shap_values_cv]),\n",
    "  feature_names=shap_values_cv[0].feature_names,\n",
    "  compute_time=np.sum([sh.compute_time for sh in shap_values_cv]),\n",
    "  output_names=y_encoder.classes_,\n",
    "  output_indexes=y_pred,\n",
    ")\n",
    "\n",
    "\n",
    "shap.plots.beeswarm(shap_values, show=False)\n",
    "plt.suptitle('CV-SHAP values of the latent factors in {} {} dataset'.format(ATLAS, CONNECTIVITY_MEASURE))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(deepcopy(shap_values), plot_type='bar', show=False)\n",
    "\n",
    "plt.suptitle('CV-SHAP values of the latent factors in {} {} dataset'.format(ATLAS, CONNECTIVITY_MEASURE))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation Importance\n",
    "\n",
    "Another feature importance method is the permutation analysis, where we permute the feature values and compute the classification score. The effect of a feature on the classification is then plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "opt.best_estimator_.fit(X_cfa, y)\n",
    "perm_imp_result = permutation_importance(opt.best_estimator_, X_cfa, y, \n",
    "                                         n_repeats=100,\n",
    "                                         scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "perm_sorted_idx = perm_imp_result.importances_mean.argsort()\n",
    "\n",
    "perm_df = pd.DataFrame(perm_imp_result.importances[perm_sorted_idx].T,\n",
    "             columns=factors_model.keys())\n",
    "sns.boxplot(\n",
    "    data=perm_df,\n",
    "    orient='horizontal',\n",
    "#     labels=feature_names[perm_sorted_idx],\n",
    ")\n",
    "\n",
    "plt.suptitle('Permutation importance of the latent factors in {} {} dataset'.format(ATLAS, CONNECTIVITY_MEASURE))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34f04479ffaeb5c00adb9e28a92647dce776275bf5ee61de72266754f4451f1a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('acnets')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
