{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Ceiling Analysis\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "This notebook requires an additional package, `noiseceiling`.\n",
    "To run this notebook, you need to have a few packages installed. The easiest way to do this is to use mamba to create a new environment from the `environment.yml` file in the root of this repository:\n",
    "\n",
    "```bash\n",
    "mamba env create -f environment.yml\n",
    "mamba activate acnets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# 0. SETUP\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from src.acnets.pipeline import ConnectivityPipeline, ConnectivityVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.acnets.pipeline import Parcellation\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "These parameters can be set in the command line when running the notebook, or in the notebook itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "N_CV_SPLITS = 100                       # number of cross-validation splits\n",
    "N_TEST_SUBJECTS = 8                     # test size for cross-validation (number of subjects)\n",
    "\n",
    "MODELS_DIR= Path('models/')             # Directory to save models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Here we load the data from the `data/julia2018/` dataset. These files contain the connectivity matrices for each participant, for each combination of parcellation and connectivity metric. For the reminder of this notebook, we only focus on `dosenbach2010` parcellation atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# DATA PREPARATION\n",
    "parcellation = Parcellation(atlas_name='dosenbach2010').fit()\n",
    "\n",
    "subjects = parcellation.dataset_.coords['subject'].values\n",
    "\n",
    "# extract group labels (AVGP or NVGP) from subject ids (e.g. AVGP-01)\n",
    "subject_labels = [s[:4] for s in subjects]  \n",
    "\n",
    "X = subjects.reshape(-1, 1)  # subject ids, shape: (n_subjects, 1)\n",
    "\n",
    "y_encoder = LabelEncoder()\n",
    "y = y_encoder.fit_transform(subject_labels)     # labels, shape: (n_subjects,)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "The feature extraction pipeline is composed of the following steps:\n",
    "\n",
    "1. Extract connectivity matrices from the data\n",
    "2. Vectorize the connectivity matrices\n",
    "3. Scale the connectivity matrices\n",
    "4. Remove zero-variance features\n",
    "5. Select the top features based on the coefficient of a SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "There are no repeats in your data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m#%pip install /Users/morteza/workspace/noiseceiling\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnoiseceiling\u001b[39;00m \u001b[39mimport\u001b[39;00m compute_nc_classification\n\u001b[0;32m---> 20\u001b[0m compute_nc_classification(pd\u001b[39m.\u001b[39;49mDataFrame(X_features), pd\u001b[39m.\u001b[39;49mSeries(y))\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/noiseceiling/core.py:43\u001b[0m, in \u001b[0;36mcompute_nc_classification\u001b[0;34m(X, y, use_repeats_only, soft, per_class, use_index, score_func, progress_bar)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_nc_classification\u001b[39m(X, y, use_repeats_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, soft\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, per_class\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m                               use_index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, score_func\u001b[39m=\u001b[39mroc_auc_score, progress_bar\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m      9\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Computes a noise ceiling for classification models (i.e., when\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m    the dependent variable is categorical).\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m        the columns represent the different classes).\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     X_, y \u001b[39m=\u001b[39m _check_Xy(X, y, categorical\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, use_index\u001b[39m=\u001b[39;49muse_index)\n\u001b[1;32m     44\u001b[0m     \u001b[39mif\u001b[39;00m use_index:\n\u001b[1;32m     45\u001b[0m         X_ \u001b[39m=\u001b[39m _use_index(X)\n",
      "File \u001b[0;32m~/micromamba/envs/acnets/lib/python3.10/site-packages/noiseceiling/utils.py:102\u001b[0m, in \u001b[0;36m_check_Xy\u001b[0;34m(X, y, categorical, use_index)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[39massert\u001b[39;00m(y\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mequals(X\u001b[39m.\u001b[39mindex))\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mduplicated()\u001b[39m.\u001b[39msum() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThere are no repeats in your data.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39mif\u001b[39;00m categorical:\n",
      "\u001b[0;31mValueError\u001b[0m: There are no repeats in your data."
     ]
    }
   ],
   "source": [
    "# DEFINE PIPELINE\n",
    "\n",
    "pipe  = Pipeline([\n",
    "    ('connectivity', ConnectivityPipeline(kind='partial correlation')),\n",
    "    ('vectorize', ConnectivityVectorizer()),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('zerovar', VarianceThreshold()),\n",
    "    ('select', SelectFromModel(LinearSVC(penalty='l1', dual=False, max_iter=10000),\n",
    "                               max_features=lambda x: min(10, x.shape[1]))),\n",
    "    # ('clf', SVC(kernel='linear', C=1))\n",
    "])\n",
    "\n",
    "# DEBUG (expected to overfit, i.e., score=1)\n",
    "X_features = pipe.fit_transform(X, y)\n",
    "\n",
    "# WARNING the noiseceiling does not work out of the box, download the source code, change `sklearn` dependency to `scikit-learn` in `setup.py` and install with `pip install .` in the source code directory. See https://github.com/lukassnoek/noiseceiling for the original source code.\n",
    "\n",
    "from noiseceiling import compute_nc_classification\n",
    "compute_nc_classification(pd.DataFrame(X_features), pd.Series(y))\n",
    "\n",
    "# RESULT: the noise ceiling analysis is not applicable to this dataset because there is no repeat in the X."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the pipeline\n",
    "\n",
    "Here we verify that the pipeline works by running it on all aggregation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timeseries=None, connectivity=None]\n",
      "Test accuracy (mean ± std): 0.47 ± 0.16\n",
      "ConfidenceInterval(low=0.44125, high=0.505) \n",
      "\n",
      "[timeseries=network, connectivity=None]\n",
      "Test accuracy (mean ± std): 0.74 ± 0.14\n",
      "ConfidenceInterval(low=0.7075, high=0.76375) \n",
      "\n",
      "[timeseries=random_network, connectivity=None]\n",
      "Test accuracy (mean ± std): 0.49 ± 0.13\n",
      "ConfidenceInterval(low=0.46, high=0.51) \n",
      "\n",
      "[timeseries=None, connectivity=network]\n",
      "Test accuracy (mean ± std): 0.50 ± 0.19\n",
      "ConfidenceInterval(low=0.46625, high=0.54) \n",
      "\n",
      "[timeseries=None, connectivity=random_network]\n",
      "Test accuracy (mean ± std): 0.50 ± 0.15\n",
      "ConfidenceInterval(low=0.46875, high=0.525) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST VARIOUS AGGREGATIONS (calculate cross-validated accuracy and bootstrap CI)\n",
    "\n",
    "for timeseries_aggregation, connectivity_aggregation in [\n",
    "    (None, None),                # no aggregation (regions)\n",
    "    ('network', None),           # time-series aggregation region->network\n",
    "    ('random_network', None),    # time-series aggregation region->random_network\n",
    "    (None, 'network'),           # connectivity matrix aggregation region->network\n",
    "    (None, 'random_network'),    # connectivity matrix aggregation region->random_network\n",
    "    ]:\n",
    "\n",
    "    pipe.set_params(connectivity__atlas='dosenbach2010',\n",
    "                    connectivity__kind='partial correlation',\n",
    "                    connectivity__timeseries_aggregation=timeseries_aggregation,\n",
    "                    connectivity__connectivity_aggregation=connectivity_aggregation)\n",
    "\n",
    "    scores = cross_val_score(pipe, X, y,\n",
    "                            cv=CV,\n",
    "                            scoring='accuracy',\n",
    "                            n_jobs=-1)\n",
    "    bootstrap_ci = stats.bootstrap(scores.reshape(1,-1), np.mean)\n",
    "\n",
    "    print(f'[timeseries={timeseries_aggregation}, connectivity={connectivity_aggregation}]')\n",
    "    print('Test accuracy (mean ± std): {:.2f} ± {:.2f}'.format(scores.mean(), scores.std()))\n",
    "    print(bootstrap_ci.confidence_interval, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_name(params):\n",
    "    \"\"\"Helper function to generate a unique model name from the parameters.\"\"\"\n",
    "\n",
    "    atlas = params['connectivity__atlas']\n",
    "    kind = params['connectivity__kind'].replace(' ', '')\n",
    "    tagg = params['connectivity__timeseries_aggregation'] or 'region'  # none = region\n",
    "    cagg = params['connectivity__connectivity_aggregation'] or 'none'  # none = ts-aggregation\n",
    "    tagg = tagg.replace('random_network', 'random')  # random_network -> random\n",
    "    cagg = cagg.replace('random_network', 'random')  # random_network -> random\n",
    "    name = f'{atlas}_kind-{kind}_tagg-{tagg}_cagg-{cagg}'\n",
    "\n",
    "    return name\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('acnets')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27337377eeae3c8189a7b021b93003f903d6e200a1ea36bbed16bbe086d62899"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
