{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiScale Classifier\n",
    "\n",
    "Sections:\n",
    "\n",
    "1. Data\n",
    "2. Hyper-parameter space\n",
    "3. HPO\n",
    "4. Cross-validation scores\n",
    "5. Feature Importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "from ray import tune\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import skexplain\n",
    "from skexplain.common.importance_utils import to_skexplain_importance\n",
    "\n",
    "from src.acnets.pipeline import MultiScaleClassifier, Parcellation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUNS = 100      # 10 independent train/test runs\n",
    "TEST_SIZE = .25  # proportion of test subjects out of 32 subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] label mapping: {'AVGP': 0, 'NVGP': 1}\n",
      "[DEBUG] overfit accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Xy\n",
    "subjects = Parcellation(atlas_name='difumo_64_2mm').fit_transform(None).coords['subject'].values\n",
    "X = subjects.reshape(-1,1)                                  # subjects ids, shape: (n_subjects, 1)\n",
    "\n",
    "y_encoder = LabelEncoder()\n",
    "y = y_encoder.fit_transform([s[:4] for s in subjects])      # labels (AVGP=1 or NVGP=1), shape: (n_subjects,)\n",
    "y_mapping = dict(zip(y_encoder.classes_, y_encoder.transform(y_encoder.classes_)))\n",
    "\n",
    "# DEBUG (report label mapping)\n",
    "print('[DEBUG] label mapping:', y_mapping)\n",
    "\n",
    "# DEBUG (expected to overfit, i.e., accuracy is 1)\n",
    "overfit_score = MultiScaleClassifier().fit(X, y).score(X, y)\n",
    "print(f'[DEBUG] overfit accuracy: {overfit_score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_space = {\n",
    "    # 'atlas': ['dosenbach2010', 'gordon2014_2mm', 'difumo_64_2mm'],\n",
    "    'atlas': tune.choice(['dosenbach2010']),\n",
    "    'extract_h1_features': tune.grid_search([True]),\n",
    "    'extract_h2_features': tune.grid_search([True]),\n",
    "    'extract_h3_features': tune.grid_search([True]),\n",
    "    # 'clf__subsample': tune.choice([.5, .8, 1]),\n",
    "    'clf__n_estimators': tune.grid_search([100, 200]),\n",
    "    'clf__max_depth': tune.grid_search([2, 4, 6, 8]),\n",
    "    'clf__learning_rate': tune.grid_search([.1, .3]),\n",
    "    # 'clf__colsample_bytree': tune.uniform(0.1, 1.0),\n",
    "    # 'clf__colsample_bylevel': tune.uniform(0.1, 1.0),\n",
    "    # TODO 'clf__bagging_fraction': tune.uniform(0.01, 1.0),\n",
    "    # TODO 'clf__min_child_weight': tune.randint(1, 10),\n",
    "}\n",
    "\n",
    "rfc_param_space = {\n",
    "    # 'clf': RandomForestClassifier(),\n",
    "    # 'atlas': tune.choice(['dosenbach2010', 'gordon2014_2mm', 'difumo_64_2mm']),\n",
    "    'atlas': tune.choice(['dosenbach2010']),\n",
    "    'clf__n_estimators': tune.randint(100, 500),\n",
    "    'clf__max_depth': tune.randint(1, 8),\n",
    "    'clf__min_samples_split': tune.randint(2, 8),\n",
    "    'clf__min_samples_leaf': tune.randint(1, 5),\n",
    "    'clf__criterion': tune.choice(['gini', 'entropy']),\n",
    "    'clf__max_features': tune.choice([None, 'sqrt'])\n",
    "}\n",
    "\n",
    "svm_param_space = {\n",
    "    # 'clf': LinearSVC(max_iter=100000),\n",
    "    # 'atlas': tune.choice(['dosenbach2010', 'gordon2014_2mm', 'difumo_64_2mm']),\n",
    "    'atlas': tune.choice(['dosenbach2010']),\n",
    "    'clf__penalty': ['l1'],\n",
    "    'clf__dual': [False],\n",
    "    'clf__C': tune.choice([.01, .1, 1, 10, 100, 1000]),\n",
    "    # 'clf__kernel': ['linear','poly','rbf','sigmoid'],\n",
    "    # 'clf__gamma': tune.choice(['scale'])\n",
    "}\n",
    "\n",
    "def eval_model(config,\n",
    "               X=X, y=y,\n",
    "               n_runs=1,\n",
    "               classifier=XGBClassifier(base_score=.5, objective='binary:logistic')):\n",
    "\n",
    "    val_scores = []\n",
    "    test_scores = []\n",
    "\n",
    "    model = MultiScaleClassifier(classifier=classifier).set_params(**config)\n",
    "\n",
    "    for i in range(n_runs):\n",
    "\n",
    "        # outer CV\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, stratify=y)\n",
    "\n",
    "        # Inner CV (K-fold)\n",
    "        inner_cv = StratifiedKFold(n_splits=8, shuffle=True)\n",
    "\n",
    "        # fit and score the validation set\n",
    "        val_score = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=inner_cv).mean()\n",
    "\n",
    "        # test score (we only report this and do not use it during HPO)\n",
    "        test_score = model.fit(X_train, y_train).score(X_test, y_test)\n",
    "\n",
    "        val_scores.append(val_score)\n",
    "        test_scores.append(test_score)\n",
    "\n",
    "    return {'val_accuracy': np.mean(val_scores),\n",
    "            'test_accuracy': np.mean(test_scores)}\n",
    "\n",
    "# DEBUG\n",
    "# config = dict(atlas='dosenbach2010', extract_h1_features=False, extract_h2_features=True)\n",
    "# 'DEBUG', eval_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-10-03 12:59:07</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:05.22        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.1/62.7 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 8.0/8 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th>atlas        </th><th style=\"text-align: right;\">  clf__learning_rate</th><th style=\"text-align: right;\">  clf__max_depth</th><th style=\"text-align: right;\">  clf__n_estimators</th><th>extract_h1_features  </th><th>extract_h2_features  </th><th>extract_h3_features  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>eval_model_dcbf3_00000</td><td>RUNNING </td><td>10.184.42.11:122225</td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">               2</td><td style=\"text-align: right;\">                100</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "<tr><td>eval_model_dcbf3_00001</td><td>RUNNING </td><td>10.184.42.11:122226</td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">               2</td><td style=\"text-align: right;\">                100</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "<tr><td>eval_model_dcbf3_00002</td><td>RUNNING </td><td>10.184.42.11:122227</td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">               4</td><td style=\"text-align: right;\">                100</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "<tr><td>eval_model_dcbf3_00003</td><td>RUNNING </td><td>10.184.42.11:122228</td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">               4</td><td style=\"text-align: right;\">                100</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "<tr><td>eval_model_dcbf3_00004</td><td>RUNNING </td><td>10.184.42.11:122229</td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">               6</td><td style=\"text-align: right;\">                100</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "<tr><td>eval_model_dcbf3_00005</td><td>RUNNING </td><td>10.184.42.11:122230</td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">               6</td><td style=\"text-align: right;\">                100</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "<tr><td>eval_model_dcbf3_00006</td><td>RUNNING </td><td>10.184.42.11:122231</td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">               8</td><td style=\"text-align: right;\">                100</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "<tr><td>eval_model_dcbf3_00007</td><td>RUNNING </td><td>10.184.42.11:122232</td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">               8</td><td style=\"text-align: right;\">                100</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "<tr><td>eval_model_dcbf3_00008</td><td>PENDING </td><td>                   </td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">               2</td><td style=\"text-align: right;\">                200</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "<tr><td>eval_model_dcbf3_00009</td><td>PENDING </td><td>                   </td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">               2</td><td style=\"text-align: right;\">                200</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "<tr><td>eval_model_dcbf3_00010</td><td>PENDING </td><td>                   </td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">               4</td><td style=\"text-align: right;\">                200</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "<tr><td>eval_model_dcbf3_00011</td><td>PENDING </td><td>                   </td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">               4</td><td style=\"text-align: right;\">                200</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "<tr><td>eval_model_dcbf3_00012</td><td>PENDING </td><td>                   </td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">               6</td><td style=\"text-align: right;\">                200</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "<tr><td>eval_model_dcbf3_00013</td><td>PENDING </td><td>                   </td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">               6</td><td style=\"text-align: right;\">                200</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "<tr><td>eval_model_dcbf3_00014</td><td>PENDING </td><td>                   </td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">               8</td><td style=\"text-align: right;\">                200</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "<tr><td>eval_model_dcbf3_00015</td><td>PENDING </td><td>                   </td><td>dosenbach2010</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">               8</td><td style=\"text-align: right;\">                200</td><td>True                 </td><td>True                 </td><td>True                 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select the hyper-parameter space\n",
    "param_space = xgb_param_space\n",
    "\n",
    "ray.shutdown(); ray.init()\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    eval_model,\n",
    "    param_space=param_space,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric='val_accuracy',\n",
    "        mode='max',\n",
    "    )\n",
    ")\n",
    "\n",
    "tuning_results = tuner.fit()\n",
    "ray.shutdown()\n",
    "\n",
    "clear_output()\n",
    "print('[DEBUG] Best HPO score:',\n",
    "      tuner.get_results().get_best_result().metrics['val_accuracy'])\n",
    "\n",
    "# create a tuned model using the best hyper-parameters\n",
    "best_params = tuner.get_results().get_best_result(metric='val_accuracy', mode='max').config\n",
    "tuned_model = MultiScaleClassifier(classifier=XGBClassifier()).set_params(**best_params)\n",
    "tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(tuned_model, X, y,\n",
    "                            cv=StratifiedShuffleSplit(n_splits=N_RUNS, test_size=TEST_SIZE),\n",
    "                            verbose=3, n_jobs=-1)\n",
    "\n",
    "# calculate 95% confidence interval\n",
    "bootstrap_ci = stats.bootstrap(cv_scores.reshape(1,-1), np.mean)\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(f'Test accuracy (mean ± std): {cv_scores.mean():.2f} ± {cv_scores.std():.2f}')\n",
    "print(bootstrap_ci.confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "We implement three methods for feature importance:\n",
    "- permutation importance\n",
    "- weights from the fitted classifier\n",
    "- Multipass and SHAP\n",
    "- DropCol (custom-made)\n",
    "\n",
    "It either uses weights from the fitted classifier or calculates permutation importance. The latter is more computationally expensive but more reliable for all kinds of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Feature Importance\n",
    "\n",
    "def get_importance(estimator, X_train, y_train, X_test, y_test, scoring='accuracy', use_coef=False):\n",
    "    \"\"\"Perform permutation importance analysis on a given estimator.\"\"\"\n",
    "\n",
    "    estimator.fit(X_train, y_train)\n",
    "    feature_names = estimator.get_feature_extractor().get_feature_names_out()\n",
    "    selected_feature_names = estimator[2:-1].get_feature_names_out(feature_names)\n",
    "\n",
    "    if use_coef and 'xgb' in estimator.named_steps['clf'].__class__.__name__.lower():\n",
    "        # for xgb classifier we use its booster's get_score() method\n",
    "        booster = estimator.named_steps['clf'].get_booster()\n",
    "        booster.feature_names = selected_feature_names.tolist()\n",
    "        importance = booster.get_score(importance_type='weight', fmap='')\n",
    "    elif use_coef and 'feature_importances_' in dir(estimator.named_steps['clf']):\n",
    "        # for some other classifiers we use their feature_importances_ attribute\n",
    "        importance = estimator.named_steps['clf'].feature_importances_\n",
    "        importance = dict(zip(feature_names, importance))\n",
    "    else:\n",
    "        # generic permutation importance\n",
    "        X_test_features = estimator.get_feature_extractor().transform(X_test)\n",
    "        feature_names = estimator.get_feature_extractor().get_feature_names_out()\n",
    "        importance = permutation_importance(estimator.get_classification_head(),\n",
    "                                            X_test_features,\n",
    "                                            y_test,\n",
    "                                            n_jobs=-1,\n",
    "                                            scoring=scoring)['importances_mean']\n",
    "        importance = dict(zip(feature_names, importance))\n",
    "\n",
    "    importance_df = pd.DataFrame([(k, v)\n",
    "                                  for k, v in importance.items()],\n",
    "                                 columns=['feature', 'importance'])\n",
    "    return importance_df\n",
    "\n",
    "\n",
    "# run permutation importance in parallel\n",
    "importances = Parallel(n_jobs=-1, verbose=2)(\n",
    "    delayed(get_importance)(\n",
    "        estimator = tuned_model,\n",
    "        X_train = X[train],\n",
    "        y_train = y[train],\n",
    "        X_test = X[test],\n",
    "        y_test = y[test])\n",
    "    for train, test in StratifiedShuffleSplit(n_splits=N_RUNS, test_size=TEST_SIZE).split(X, y)\n",
    ")\n",
    "\n",
    "# convert importance scores to dataframe and sort\n",
    "importances = pd.concat(importances, axis=0).reset_index(drop=True)\n",
    "importances['feature_group'] = importances['feature'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "# sort features by mean importance and select top-20 \n",
    "order = (importances.groupby('feature')[['importance']]\n",
    "                    .mean()\n",
    "                    .sort_values('importance', ascending=False)[:20].index)\n",
    "\n",
    "sns.barplot(data=importances,\n",
    "            order=order,  \n",
    "            x='importance',\n",
    "            y='feature',\n",
    "            errorbar=('ci', 95),\n",
    "            orient='h')\n",
    "\n",
    "plt.xlabel('Importance (average weight of tree-splits which use the feature)')\n",
    "plt.show()\n",
    "\n",
    "# plot feature-group importance\n",
    "sns.barplot(data=importances,\n",
    "            x='importance',\n",
    "            y='feature_group',\n",
    "            errorbar=('ci', 95),\n",
    "            orient='h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multipass & SHAP Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-explain\n",
    "\n",
    "\n",
    "# Loads three ML models (random forest, gradient-boosted tree, and logistic regression)\n",
    "# trained on a subset of the road surface temperature data from Handler et al. (2020).\n",
    "\n",
    "estimator = tuned_model.fit(X, y)\n",
    "X_features = estimator.get_feature_extractor().transform(X)\n",
    "feature_names = estimator.get_feature_extractor().get_feature_names_out()\n",
    "\n",
    "explainer = skexplain.ExplainToolkit(('xgb', estimator.get_classification_head()),\n",
    "                                     estimator_output='raw',\n",
    "                                     X=X_features,y=y, feature_names=feature_names)\n",
    "\n",
    "\n",
    "# SHAP\n",
    "shap_results = explainer.local_attributions(method='shap')['shap_values__xgb'].values\n",
    "shap_data = to_skexplain_importance(shap_results, estimator_name='xgb', \n",
    "                                      feature_names=feature_names, method='shap_sum')\n",
    "explainer.plot_importance(data=shap_data,\n",
    "                          panels = [('shap_sum', 'xgb')],\n",
    "                          plot_correlated_features=True,\n",
    "                          rho_threshold=0.7,\n",
    "                          figsize=(4, 2.5),\n",
    "                          method='shap')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# multipass\n",
    "perm_results = explainer.permutation_importance(n_vars=10, direction='forward', evaluation_fn='auc')\n",
    "explainer.plot_importance(data=perm_results,\n",
    "                          panels=[('forward_multipass', 'xgb')],\n",
    "                          plot_correlated_features=True,\n",
    "                          rho_threshold=0.7,\n",
    "                          figsize=(4, 2.5),\n",
    "                          )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop-Column Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DropCol feature importance\n",
    "\n",
    "def get_dropcol_importance(estimator, X_train, y_train, X_test, y_test, scoring='accuracy'):\n",
    "\n",
    "    estimator.fit(X, y)  # just fit if not already fitted\n",
    "    feature_names = estimator.get_feature_extractor().get_feature_names_out()\n",
    "    X_train_features = estimator.get_feature_extractor().transform(X_train)\n",
    "    X_test_features = estimator.get_feature_extractor().transform(X_test)\n",
    "    clf = estimator.get_classification_head()\n",
    "\n",
    "    importances = []\n",
    "    for i, feat_name in enumerate(feature_names):\n",
    "        X_train_drop = np.delete(X_train_features, i, axis=1)\n",
    "        X_test_drop = np.delete(X_test_features, i, axis=1)\n",
    "        clf.fit(X_train_drop, y_train)\n",
    "        importance = permutation_importance(clf,\n",
    "                                            X_test_drop,\n",
    "                                            y_test,\n",
    "                                            n_jobs=-1,\n",
    "                                            scoring=scoring)['importances_mean']\n",
    "        new_feature_names = list(feature_names).copy()\n",
    "        new_feature_names.remove(feat_name)\n",
    "        importance = list(zip(new_feature_names, importance))\n",
    "        importances.extend(importance)\n",
    "\n",
    "    return importances\n",
    "\n",
    "# run permutation importance in parallel\n",
    "importances = Parallel(n_jobs=-1, verbose=2)(\n",
    "    delayed(get_dropcol_importance)(\n",
    "        estimator = tuned_model,\n",
    "        X_train = X[train],\n",
    "        y_train = y[train],\n",
    "        X_test = X[test],\n",
    "        y_test = y[test])\n",
    "    for train, test in StratifiedShuffleSplit(n_splits=N_RUNS, test_size=TEST_SIZE).split(X, y)\n",
    ")\n",
    "pd.DataFrame([item for sublist in importances for item in sublist], columns=['feature', 'importance']).groupby('feature').mean().sort_values('importance', ascending=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
