{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "from src.acnets.pipeline import Parcellation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from tqdm import tqdm\n",
    "from joblib import delayed, Parallel\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from src.acnets.pipeline import MultiScaleClassifier\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "from ray.tune.sklearn import TuneSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input/Output\n",
    "parcellation = Parcellation(atlas_name='dosenbach2010')\n",
    "subjects = parcellation.fit_transform(None).coords['subject'].values\n",
    "subject_labels = [s[:4] for s in subjects]\n",
    "X = subjects.reshape(-1,1)                     # subjects, shape: (n_subjects, 1)\n",
    "y_encoder = LabelEncoder()\n",
    "y = y_encoder.fit_transform(subject_labels)     # labels, shape: (n_subjects,)\n",
    "\n",
    "\n",
    "# DEBUG (expected to overfit, i.e., score=1)\n",
    "overfit_score = MultiScaleClassifier().fit(X, y).score(X, y)\n",
    "print(f'[DEBUG] overfit accuracy: {overfit_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_space = {\n",
    "    'clf__objective': ['binary:logistic'],\n",
    "    'clf__max_depth': tune.randint(1, 20),\n",
    "    'clf__min_child_weight': tune.randint(1, 10),\n",
    "    'clf__subsample': tune.uniform(0.01, 1.0),\n",
    "    'clf__eta': tune.loguniform(1e-4, 1e-1),\n",
    "    'clf__learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'clf__n_estimators': tune.choice([25, 50, 100, 1000]),\n",
    "    # 'clf__max_depth': (0, 50),\n",
    "    # 'clf__max_delta_step': (0, 20),\n",
    "    # 'clf__reg_lambda': (1e-9, 1000, 'log-uniform'),\n",
    "    # 'clf__reg_alpha': (1e-9, 1.0, 'log-uniform'),\n",
    "    # 'clf__gamma': (1e-9, 0.5, 'log-uniform'),\n",
    "    # 'clf__scale_pos_weight': (1e-6, 500, 'log-uniform')\n",
    "}\n",
    "\n",
    "tuner = TuneSearchCV(\n",
    "    MultiScaleClassifier(),\n",
    "    HyperOptSearch.convert_search_space(param_space),\n",
    "    scoring='accuracy',\n",
    "    cv=StratifiedShuffleSplit(n_splits=5, test_size=16),\n",
    "    search_optimization='hyperopt',\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    n_trials=10,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "tuner.fit(X, y)\n",
    "clear_output()\n",
    "\n",
    "# create a model with best params\n",
    "MultiScaleClassifier().set_params(**tuner.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=10, test_size=8)\n",
    "\n",
    "model = MultiScaleClassifier().set_params(**tuner.best_params_)\n",
    "cv_scores = cross_val_score(model, X, y, cv=cv, verbose=3, n_jobs=-1)\n",
    "bootstrap_ci = stats.bootstrap(cv_scores.reshape(1,-1), np.mean)\n",
    "\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(f'Test accuracy (mean ± std): {cv_scores.mean():.2f} ± {cv_scores.std():.2f}')\n",
    "print(bootstrap_ci.confidence_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Feature Importance\n",
    "\n",
    "model = MultiScaleClassifier().set_params(**tuner.best_params_)\n",
    "feature_names = model.fit(X, y).get_feature_names_out()\n",
    "\n",
    "def do_permutation_importance(estimator, X_train, y_train, X_test, y_test, scoring='accuracy'):\n",
    "    \"\"\"Perform permutation importance analysis on a given estimator.\"\"\"\n",
    "    estimator.fit(X_train, y_train)\n",
    "    X_test_features = estimator.get_feature_extractor().transform(X_test)\n",
    "    results = permutation_importance(estimator.get_classification_head(),\n",
    "                                     X_test_features,\n",
    "                                     y_test,\n",
    "                                     n_jobs=-1,\n",
    "                                     scoring=scoring)\n",
    "    return results['importances_mean']\n",
    "\n",
    "\n",
    "# run permutation importance in parallel\n",
    "permutation_cv = StratifiedShuffleSplit(n_splits=10, test_size=8)\n",
    "importance_scores = Parallel(n_jobs=-1, verbose=2)(\n",
    "    delayed(do_permutation_importance)(\n",
    "        estimator = model,\n",
    "        X_train = X[train],\n",
    "        y_train = y[train],\n",
    "        X_test = X[test],\n",
    "        y_test = y[test])\n",
    "    for train, test in permutation_cv.split(X, y)\n",
    ")\n",
    "\n",
    "# convert to dataframe and sort\n",
    "importance_scores = pd.DataFrame(\n",
    "    data=np.stack(importance_scores, axis=0),\n",
    "    columns=feature_names).mean().sort_values(ascending=False).to_frame('importance')\n",
    "\n",
    "importance_scores[:20]  # top 20 featuresp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
