{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "from IPython.display import clear_output\n",
    "from joblib import Parallel, delayed\n",
    "from ray import tune\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.sklearn import TuneSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from src.acnets.pipeline import MultiScaleClassifier, Parcellation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUNS = 100     # 10 independent train/test runs\n",
    "TEST_SIZE = .25  # 8 subjects out of 32 subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xy\n",
    "subjects = Parcellation(atlas_name='difumo_64_2mm').fit_transform(None).coords['subject'].values\n",
    "X = subjects.reshape(-1,1)                                  # subjects ids, shape: (n_subjects, 1)\n",
    "\n",
    "y_encoder = LabelEncoder()\n",
    "y = y_encoder.fit_transform([s[:4] for s in subjects])      # labels (AVGP=1 or NVGP=1), shape: (n_subjects,)\n",
    "y_mapping = dict(zip(y_encoder.classes_, y_encoder.transform(y_encoder.classes_)))\n",
    "\n",
    "# DEBUG (report label mapping)\n",
    "print('[DEBUG] label mapping:', y_mapping)\n",
    "\n",
    "# DEBUG (expected to overfit, i.e., accuracy is 1)\n",
    "overfit_score = MultiScaleClassifier().fit(X, y).score(X, y)\n",
    "print(f'[DEBUG] overfit accuracy: {overfit_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown(); ray.init()\n",
    "\n",
    "xgb_config = {\n",
    "    'clf': XGBClassifier(),\n",
    "    'params': {\n",
    "        'atlas': ['dosenbach2010', 'gordon2014_2mm', 'difumo_64_2mm'],\n",
    "        'clf__base_score': [.5],\n",
    "        'clf__objective': ['binary:logistic'],\n",
    "        'clf__n_estimators': tune.randint(100, 500),\n",
    "        'clf__max_depth': tune.randint(1, 8),\n",
    "        'clf__learning_rate': tune.uniform(0.01, 0.1),\n",
    "        # TODO 'clf__colsample_bytree': tune.uniform(0.01, 1.0),\n",
    "        # TODO 'clf__bagging_fraction': tune.uniform(0.01, 1.0),\n",
    "        # TODO 'clf__min_child_weight': tune.randint(1, 10),\n",
    "    }\n",
    "}\n",
    "\n",
    "rfc_config = {\n",
    "    'clf': RandomForestClassifier(),\n",
    "    'params': {\n",
    "        'atlas': ['dosenbach2010', 'gordon2014_2mm', 'difumo_64_2mm'],\n",
    "        'clf__n_estimators': tune.randint(100, 500),\n",
    "        'clf__max_depth': tune.randint(1, 8),\n",
    "        'clf__min_samples_split': tune.randint(2, 8),\n",
    "        'clf__min_samples_leaf': tune.randint(1, 5),\n",
    "        'clf__criterion': tune.choice(['gini', 'entropy']),\n",
    "        'clf__max_features': tune.choice([None, 'sqrt'])\n",
    "    }\n",
    "}\n",
    "\n",
    "svc_config = {\n",
    "    'clf': SVC(),\n",
    "    'params': {\n",
    "        'atlas': ['dosenbach2010', 'gordon2014_2mm', 'difumo_64_2mm'],\n",
    "        'clf__C': [.1, 1, 10, 100, 1000],\n",
    "        'clf__kernel': ['linear','poly','rbf','sigmoid'],\n",
    "        'clf__gamma': tune.choice(['scale'])\n",
    "    }\n",
    "}\n",
    "\n",
    "#############################################\n",
    "# HPO\n",
    "#############################################\n",
    "\n",
    "config = xgb_config\n",
    "\n",
    "tuner = TuneSearchCV(\n",
    "    MultiScaleClassifier(classifier=config['clf']),\n",
    "    HyperOptSearch.convert_search_space(config['params']),\n",
    "    scoring='accuracy',\n",
    "    cv=StratifiedShuffleSplit(n_splits=N_RUNS, test_size=TEST_SIZE),\n",
    "    search_optimization='hyperopt',\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    n_trials=10,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "tuner.fit(X, y)\n",
    "ray.shutdown()\n",
    "\n",
    "clear_output()\n",
    "print('[DEBUG] Best HPO score:', tuner.best_score_)\n",
    "\n",
    "# create a tuned model using the best hyper-parameters\n",
    "tuned_model = MultiScaleClassifier(atlas=tuner.best_params_['atlas'],\n",
    "                                   classifier=config['clf']\n",
    "                                   ).set_params(**tuner.best_params_)\n",
    "\n",
    "tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(estimator = tuned_model,\n",
    "                            X=X,\n",
    "                            y=y,\n",
    "                            cv=StratifiedShuffleSplit(n_splits=N_RUNS, test_size=TEST_SIZE),\n",
    "                            verbose=3,\n",
    "                            n_jobs=-1)\n",
    "bootstrap_ci = stats.bootstrap(cv_scores.reshape(1,-1), np.mean)\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(f'Test accuracy (mean ± std): {cv_scores.mean():.2f} ± {cv_scores.std():.2f}')\n",
    "print(bootstrap_ci.confidence_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Feature Importance\n",
    "\n",
    "feature_names = tuned_model.fit(X, y).get_feature_names_out()\n",
    "\n",
    "def do_permutation_importance(estimator, X_train, y_train, X_test, y_test, scoring='accuracy'):\n",
    "    \"\"\"Perform permutation importance analysis on a given estimator.\"\"\"\n",
    "    estimator.fit(X_train, y_train)\n",
    "    X_test_features = estimator.get_feature_extractor().transform(X_test)\n",
    "    results = permutation_importance(estimator.get_classification_head(),\n",
    "                                     X_test_features,\n",
    "                                     y_test,\n",
    "                                     n_jobs=-1,\n",
    "                                     scoring=scoring)\n",
    "    return results['importances_mean']\n",
    "\n",
    "\n",
    "# run permutation importance in parallel\n",
    "importance_scores = Parallel(n_jobs=-1, verbose=2)(\n",
    "    delayed(do_permutation_importance)(\n",
    "        estimator = tuned_model,\n",
    "        X_train = X[train],\n",
    "        y_train = y[train],\n",
    "        X_test = X[test],\n",
    "        y_test = y[test])\n",
    "    for train, test in StratifiedShuffleSplit(n_splits=N_RUNS, test_size=TEST_SIZE).split(X, y)\n",
    ")\n",
    "\n",
    "# convert importance scores to dataframe and sort\n",
    "importance_scores = pd.DataFrame(\n",
    "    data=np.stack(importance_scores, axis=0),\n",
    "    columns=feature_names).mean().sort_values(ascending=False).to_frame('importance')\n",
    "\n",
    "# DEBUG: report top 20 features\n",
    "importance_scores[:20]  # top 20 features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
